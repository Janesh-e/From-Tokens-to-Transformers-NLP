
# ğŸ§© TOKENIZATION in NLP :

### âœ… What is Tokenization?

Tokenization is the process of **breaking text into smaller units called tokens**.  
Tokens can be:

- **Words**
- **Subwords**
- **Characters**
- **Sentences**

> ğŸ“Œ Think of tokenization as "segmenting a paragraph into Lego blocks" so that computers can process each block meaningfully.

---

### ğŸ¤– Why Tokenization is Needed

##### ğŸ§  Models cannot read raw text

- Computers donâ€™t understand "language" â€” they understand numbers.
- Tokenization prepares text so it can be **converted to vectors**, one token at a time.

##### ğŸ” NLP tasks depend on it

- All NLP models â€” whether rule-based, classical ML, or deep learning â€” rely on tokenization:
    - Sentiment Analysis
    - Chatbots
    - Translation
    - Summarization
    - LLMs like GPT, BERT

---

### ğŸ§  Types of Tokenization

Letâ€™s break them down in increasing depth:

##### 1. ğŸ§± **Sentence Tokenization**

> Split a paragraph into separate sentences.

###### âœ… Why useful?

- For tasks like summarization, translation, question-answering.
- Helps model learn sentence-level semantics.

###### ğŸ§ª Code Example: `nltk`

```python
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt_tab')

text = "Hello there! I am learning NLP. Tokenization is cool."
sent_tokenize(text)
# â†’ ['Hello there!', 'I am learning NLP.', 'Tokenization is cool.']
```

###### ğŸ“š `nltk.sent_tokenize`

- **Basis**: Pre-trained **Punkt Sentence Tokenizer**, unsupervised algorithm.
- **Logic**:
    - Learns patterns of punctuation (like `.`, `!`, `?`) and abbreviations (`Dr.`, `Mr.`).
    - Handles edge cases like `"U.S.A. is a country. It is big."` correctly.
- **Language-aware**: Yes. You can load for other languages too (`punkt/english`, `punkt/german`, etc.).

```python
from nltk.tokenize import sent_tokenize sent_tokenize("Dr. Smith is here. He came at 5 p.m.") 
# ['Dr. Smith is here.', 'He came at 5 p.m.']`
```

###### ğŸ§ª Code Example: `spaCy`

```python
import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp("Hello there! I am learning NLP. Tokenization is cool.")
[sent.text for sent in doc.sents]
```

###### ğŸ“š `spaCy` Sentence Tokenization (`doc.sents`)

- **Basis**: Dependency parsing & part-of-speech tagging.
- **Logic**:
    - Uses **linguistic rules** to determine sentence boundaries.
    - Recognizes that a period may not mean end of a sentence if it follows abbreviations.
    - Faster and more syntactically informed than NLTK.

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Dr. Smith is here. He came at 5 p.m.")
[sent.text for sent in doc.sents]
```

---

##### 2. ğŸ”¤ **Word Tokenization**

> Split a sentence into individual words.

###### âœ… Why useful?

- Core for most NLP models (e.g., classification, tagging)
- Words are the **main semantic unit** in language.

###### ğŸ§ª Code Example: `nltk`

```python
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')

text = "I'm learning NLP, it's amazing!"
word_tokenize(text)
# â†’ ['I', "'m", 'learning', 'NLP', ',', 'it', "'s", 'amazing', '!']
```

> ğŸ“Œ Note: `'I'm'` becomes `['I', "'m"]` â€” this is important in LLMs.

###### ğŸ“š `nltk.word_tokenize`

- **Basis**: Uses **TreebankWordTokenizer**
- **Logic**:
    - Rule-based using regex patterns.
    - Splits contractions like `"don't"` into `["do", "n't"]`
    - Separates punctuation (`"I'm good!"` â†’ `["I", "'m", "good", "!"]`)

```python
from nltk.tokenize import word_tokenize
word_tokenize("I'm good, aren't you?")
# ['I', "'m", 'good', ',', 'are', "n't", 'you', '?']
```

###### ğŸ§ª Code Example: `spaCy`

```python
doc = nlp("I'm learning NLP, it's amazing!")
[token.text for token in doc]
# ['I', "'m", 'learning', 'NLP', ',', 'it', "'s", 'amazing', '!']
```

###### ğŸ“š `spaCy` Tokenization

- **Basis**: Rule-based + custom tokenizer
- **Logic**:
    - Uses **prefix**, **suffix**, and **infix** regex rules.
    - Handles exceptions via **special cases** (e.g., don't split "New York", keep "U.S.A.")
    - Language-specific rules (works for over 50+ languages).

```python
doc = nlp("I'm good, aren't you?")
[token.text for token in doc]
# ['I', "'m", 'good', ',', 'are', "n't", 'you', '?']
```

â¡ï¸ You can modify these rules using:

```python
nlp.tokenizer.rules
nlp.tokenizer.infix_finditer
```

---

##### 3. ğŸ§© **Subword Tokenization (BPE, WordPiece, SentencePiece)**

> Break uncommon or out-of-vocab words into **smaller parts**.

###### âœ… Why useful?

- Handles unknown, rare, or misspelled words.
- Powers most **modern LLMs**: BERT, GPT, T5, etc.

###### ğŸ”§ Types:

- **BPE (Byte Pair Encoding)** â€“ GPT models
- **WordPiece** â€“ BERT
- **Unigram / SentencePiece** â€“ T5, XLNet

###### ğŸ” Example:

Word: `unbelievable`

- **Subwords**: `["un", "believ", "able"]`

Word: `autocompletionzzz` (non-existent)

- **Subwords**: `["auto", "completion", "z", "z", "z"]`

###### ğŸ§ª Code Example: HuggingFace Tokenizer

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

tokens = tokenizer.tokenize("Unbelievable and autocompletionzzz")
print(tokens)
# ['unbelievable', 'and', 'auto', '##com', '##ple', '##tion', '##zz', '##z']
```

> âœ… `##` means continuation of previous subword.

###### ğŸ§ª Convert to IDs

```python
tokenizer.convert_tokens_to_ids(tokens)
# [23653, 1998, 8285, 9006, 10814, 3508, 13213, 2480]
```

ğŸ”¹ Step 1: **Lowercasing**

Because we're using `bert-base-uncased`, the tokenizer:

- Converts the entire input to lowercase.
- Input: `"Unbelievable and autocompletionzzz"`
- Becomes: `"unbelievable and autocompletionzzz"`

ğŸ”¹ Step 2: **WordPiece Tokenization**

BERT uses **WordPiece tokenization**, which breaks unknown or rare words into subword units from a fixed vocabulary.

Here's the breakdown of:

```python
tokens = tokenizer.tokenize("Unbelievable and autocompletionzzz")
```

Output:

```python
['unbelievable', 'and', 'auto', '##com', '##ple', '##tion', '##zz', '##z']
```

Let's interpret each token:

|Token|Explanation|
|---|---|
|`unbelievable`|Found directly in the vocab|
|`and`|Found directly|
|`auto`|Found directly|
|`##com`|Subword: suffix of a bigger word (continuation of "auto")|
|`##ple`|Continuation subword|
|`##tion`|Continuation subword|
|`##zz`|Rare suffix not in vocab as a whole word, split further|
|`##z`|Final sub-subword|

âš ï¸ The `##` prefix means â€œthis is a continuation of the previous token.â€

So, `"autocompletionzzz"` becomes:

- `"auto"` + `"##com"` + `"##ple"` + `"##tion"` + `"##zz"` + `"##z"`  
    Because `"autocompletionzzz"` as a whole is **not** in BERT's vocabulary.

ğŸ”¹ Step 3: **Token IDs**

```python
tokenizer.convert_tokens_to_ids(tokens)
```

Returns:

```python
[23653, 1998, 8285, 9006, 10814, 3508, 13213, 2480]
```

These are the integer IDs of each token from BERT's vocabulary file:

|Token|ID|
|---|---|
|unbelievable|23653|
|and|1998|
|auto|8285|
|##com|9006|
|##ple|10814|
|##tion|3508|
|##zz|13213|
|##z|2480|

These IDs are what the BERT model will actually "see" during input.


#### ğŸ“š HuggingFace `tokenizer.tokenize()` (e.g., BERT, GPT, T5)

These tokenizers **learn segmentation rules** from a dataset using algorithms like:

---

##### âš™ï¸ A. **WordPiece** (used in BERT)

- **Learns from corpus**: Common subwords are kept as vocabulary.
- **Logic**:
    - Start with full words in vocab.
    - Add subwords like â€œ##ingâ€, â€œ##edâ€, â€œ##ationâ€ to deal with rare/compound words.
    - The `##` indicates a **continuation**.

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokenizer.tokenize("unaffordable")
# ['una', '##ff', '##ord', '##able']
```

---

##### âš™ï¸ B. **Byte Pair Encoding (BPE)** â€“ used in GPT-2/3/4

- **Logic**:
    - Start with characters.
    - Iteratively merge most frequent adjacent character pairs (`'t' + 'h' â†’ 'th'`, `'th' + 'e' â†’ 'the'`).
    - Captures both words and frequent subwords (efficient vocabulary).

```python
tokenizer_gpt2 = AutoTokenizer.from_pretrained("gpt2")
tokenizer_gpt2.tokenize("autocompletionzzz")
# ['aut', 'ocom', 'pletion', 'zz', 'z']

tokenizer_gpt2.tokenize("unaffordable")
# ['un', 'aff', 'ordable']
```

---

##### âš™ï¸ C. **Unigram (SentencePiece)** â€“ used in T5, XLNet

- **Logic**:
    - Probabilistic model â€” selects tokenization that maximizes likelihood of the sentence.
    - Doesnâ€™t need spaces â€” good for Chinese, Japanese, etc.
    - Uses an optimized subset of subwords for a given sentence.

```python
import sentencepiece as spm
# spm.SentencePieceTrainer.train(...)  # Used for training your own
```

---

##### 4. ğŸ‘¶ **Character Tokenization**

> Splits every word into characters.

Useful when:

- Working with non-space-separated languages (Chinese, Japanese)
- Dealing with noisy text (tweets, typos)

```python
list("NLP is fun")
# â†’ ['N', 'L', 'P', ' ', 'i', 's', ' ', 'f', 'u', 'n']
```


- **Logic**: Just splits every string into a list of characters.
- Used in low-resource languages or misspelled/variant-rich inputs.
- Example:

```python
list("transformer")
# ['t', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r']
```

No rules, no models â€” just a plain split.

---

### ğŸ§  Tokenization Comparison

|Type|Example Input|Output|Use Case|
|---|---|---|---|
|Sentence|"Hi. I'm good!"|['Hi.', "I'm good!"]|Summarization, QA|
|Word|"I'm fine!"|['I', "'m", 'fine', '!']|Classification|
|Subword|"autocompletionzzz"|['auto', '##completion', '##z', '##z']|Transformers|
|Character|"Hi"|['H', 'i']|Spelling correction, tweets|

---

### âš ï¸ Common Challenges

1. **Contractions**: "donâ€™t" â†’ "do" + "nâ€™t" or "don" + "'t"
2. **Hyphenation**: "state-of-the-art" â€” 1 or 5 tokens?
3. **Emojis**: ğŸ˜‚ â€” remove or keep?
4. **Multi-word Expressions**: "New York" â†’ 1 token or 2?
5. **Languages without whitespace**: ä¸­æ–‡æ€ä¹ˆåŠï¼Ÿ(Chinese: no spaces)

---

### Real-World Usage

|Model|Tokenization Method|
|---|---|
|GPT (1â€“4)|Byte Pair Encoding (BPE)|
|BERT|WordPiece|
|T5|SentencePiece|
|LLaMA|SentencePiece|
|ChatGPT|Custom BPE variant|

---

### ğŸ“¦ Libraries & Tools to Know

|Library|Use|
|---|---|
|`nltk`|Classical tokenization|
|`spaCy`|Fast, accurate tokenizer w/ POS & NER|
|`transformers`|LLM-compatible subword tokenizers|
|`sentencepiece`|Train your own tokenizer|
|`tokenizers`|Fastest tokenizer backend from HuggingFace (Rust-based)|

---

### ğŸ§  Summary Flow

```text
Raw Text â†’ Sentence Tokenization â†’ Word/Subword Tokenization
                          â†“
          Optional: Lowercase, Lemmatize, Remove Stopwords
                          â†“
                  Vectorization or Embedding
```

---

### âœ… Next Steps

Would you like to:

1. ğŸ”§ Code all types (sentence, word, subword) step-by-step using `nltk`, `spaCy`, and `transformers`?
2. ğŸ§ª Build a reusable Python function or class for tokenizing text?
3. âš™ï¸ Train your own subword tokenizer using `sentencepiece` or `tokenizers`?

### ğŸ§ª Want to Go Deeper?

Would you like to:

1. **Explore how to visualize token boundaries and offsets?**
2. **Train your own subword tokenizer using `tokenizers` or `sentencepiece`?**
3. **Compare BPE vs WordPiece vs SentencePiece side by side on the same sentence?**


---
---
