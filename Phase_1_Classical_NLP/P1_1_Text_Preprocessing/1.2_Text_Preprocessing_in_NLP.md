# 🧽 TEXT PREPROCESSING in NLP :

### 📌 What is Text Preprocessing?

Text preprocessing is the act of **cleaning, structuring, and standardizing raw text data** to make it usable for downstream NLP tasks (like classification, translation, sentiment analysis, etc.).

🧠 Think of it as:

> “Prepping your raw language input so that a model (like me) can _understand_ it.”

---

### 🧱 Why is it Important?

Because:
- Raw text is **messy**: It has typos, special characters, cases, and unnecessary words.
- Machine learning models can’t understand _words_, only **numbers** — so we must _transform_ words meaningfully.

---

### 🔍 Goals of Preprocessing

1. Reduce **noise** (irrelevant or inconsistent data)
2. Normalize **language variations**
3. Prepare clean inputs for vectorization/embedding
4. Improve **model performance and generalization**

---

### 🧩 Concepts / Methods in Text Preprocessing

Here is the _complete_ list of techniques we will cover — from basic to advanced:

##### 🔤 1. **Lowercasing**

> Convert all text to lowercase to avoid “Apple” ≠ “apple”.

```python
text = "NLP Is Awesome"
text.lower()  # → "nlp is awesome"
```

---

##### 🧼 2. **Removing Noise / Punctuation / Special Characters**

> Eliminate unnecessary symbols like `@, #, $, %, !, ^, &, *, etc.`

Techniques:
- Remove punctuation
- Remove numbers
- Remove URLs, emails, HTML tags
- Remove emojis (optional)

---

##### ✂️ 3. **Tokenization**

> Breaking down sentences into words (tokens).

```python
from nltk.tokenize import word_tokenize
word_tokenize("I love learning NLP!")
# → ['I', 'love', 'learning', 'NLP', '!']
```

- **Word Tokenization**
- **Sentence Tokenization**
- **Subword Tokenization** (Byte Pair Encoding – used in BERT/GPT)

---

##### 🚫 4. **Stopword Removal**

> Stopwords = very common words like “is”, “and”, “the”, “a”, etc.

These don’t add meaning for most tasks, so we remove them.

```python
from nltk.corpus import stopwords
stopwords.words('english')
```

---

##### 🌀 5. **Stemming**

> Reduce words to their root **form** by chopping off suffixes.

```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem("running")  # → "run"
```

- Aggressive, fast, can be inaccurate.

---

##### 🌱 6. **Lemmatization**

> More intelligent than stemming. Converts word to **base dictionary form** using grammar rules.

```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize("running", pos='v')  # → "run"
```

- Slower but more accurate than stemming.

---

##### 🎭 7. **Part of Speech (POS) Tagging**

> Assign grammatical roles like noun, verb, adjective, etc.

```python
from nltk import pos_tag
pos_tag(['I', 'am', 'learning', 'NLP'])
# → [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]
```

- Helps improve lemmatization and entity recognition.

---

##### 🧠 8. **Named Entity Recognition (NER)** _(optional at preprocessing stage)_

> Extract names, dates, places, organizations.

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Elon Musk founded SpaceX in California.")
[(ent.text, ent.label_) for ent in doc.ents]
```

---

##### 🧹 9. **Removing Rare Words or Frequent Words**

> Based on frequency distribution, remove:

- Words that appear _too often_ (e.g., “also”, “just”)
- Words that appear _too rarely_ (might be noise)

---

##### 🔄 10. **Spelling Correction**

> Fix common typos: “recieve” → “receive”, “teh” → “the”.

Libraries: `textblob`, `symspellpy`, `autocorrect`

---

##### 🧪 11. **Contraction Handling**

> Expand: “don’t” → “do not”, “I’ll” → “I will”

Why? Models may learn “don’t” and “do” separately — better to standardize.

---

##### 🔄 12. **Normalization**

> Bringing similar words to a consistent form:

- “u” → “you”
- “coool” → “cool”
- “10k” → “10000”

Can include:

- Unicode normalization
- Slang replacement
- Handling elongated words

---

##### 🌐 13. **Text Cleaning Pipelines (Custom Functions)**

> Combine all steps into a function or `sklearn.Pipeline` for repeated use.

---

##### 🧾 14. **Text Preprocessing for Deep Learning / Transformers**

> Special steps:

- Padding
- Truncation
- Special tokens (CLS, SEP)
- Attention masks
- Token IDs

Handled automatically by **`transformers`** library, but must understand how they work.

---

##### 🧠 BONUS: Multilingual & Domain-Specific Preprocessing

- **Different languages** need different tokenizers, rules (e.g., Chinese, Arabic)
- **Medical/legal/finance texts** often need domain-specific stopwords or vocab.

---

### 🧰 Summary Table of Preprocessing Techniques

|Technique|Required For|Tool/Library|
|---|---|---|
|Lowercase|Uniformity|Python|
|Remove Punctuations/Noise|Clean input|`re`, `string`|
|Tokenization|Split into units|`nltk`, `spaCy`, `transformers`|
|Stopwords Removal|Remove unimportant words|`nltk`, `spaCy`|
|Stemming|Reduce to root|`nltk`, `PorterStemmer`|
|Lemmatization|Dictionary form|`WordNet`, `spaCy`|
|POS Tagging|Contextual grammar|`nltk`, `spaCy`|
|NER|Named entity detection|`spaCy`, `transformers`|
|Contractions|Expand|`contractions`, regex|
|Spelling Correction|Typo fixes|`textblob`, `autocorrect`|
|Normalization|Standardization|Custom / `textnormalizer`|
|Transformers Preprocessing|Input to models|`transformers.Tokenizer`|

---
---
