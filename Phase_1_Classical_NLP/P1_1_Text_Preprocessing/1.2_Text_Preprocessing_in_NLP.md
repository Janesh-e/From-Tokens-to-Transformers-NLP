# ðŸ§½ TEXT PREPROCESSING in NLP :

### ðŸ“Œ What is Text Preprocessing?

Text preprocessing is the act of **cleaning, structuring, and standardizing raw text data** to make it usable for downstream NLP tasks (like classification, translation, sentiment analysis, etc.).

ðŸ§  Think of it as:

> â€œPrepping your raw language input so that a model (like me) can _understand_ it.â€

---

### ðŸ§± Why is it Important?

Because:
- Raw text is **messy**: It has typos, special characters, cases, and unnecessary words.
- Machine learning models canâ€™t understand _words_, only **numbers** â€” so we must _transform_ words meaningfully.

---

### ðŸ” Goals of Preprocessing

1. Reduce **noise** (irrelevant or inconsistent data)
2. Normalize **language variations**
3. Prepare clean inputs for vectorization/embedding
4. Improve **model performance and generalization**

---

### ðŸ§© Concepts / Methods in Text Preprocessing

Here is the _complete_ list of techniques we will cover â€” from basic to advanced:

##### ðŸ”¤ 1. **Lowercasing**

> Convert all text to lowercase to avoid â€œAppleâ€ â‰  â€œappleâ€.

```python
text = "NLP Is Awesome"
text.lower()  # â†’ "nlp is awesome"
```

---

##### ðŸ§¼ 2. **Removing Noise / Punctuation / Special Characters**

> Eliminate unnecessary symbols like `@, #, $, %, !, ^, &, *, etc.`

Techniques:
- Remove punctuation
- Remove numbers
- Remove URLs, emails, HTML tags
- Remove emojis (optional)

---

##### âœ‚ï¸ 3. **Tokenization**

> Breaking down sentences into words (tokens).

```python
from nltk.tokenize import word_tokenize
word_tokenize("I love learning NLP!")
# â†’ ['I', 'love', 'learning', 'NLP', '!']
```

- **Word Tokenization**
- **Sentence Tokenization**
- **Subword Tokenization** (Byte Pair Encoding â€“ used in BERT/GPT)

---

##### ðŸš« 4. **Stopword Removal**

> Stopwords = very common words like â€œisâ€, â€œandâ€, â€œtheâ€, â€œaâ€, etc.

These donâ€™t add meaning for most tasks, so we remove them.

```python
from nltk.corpus import stopwords
stopwords.words('english')
```

---

##### ðŸŒ€ 5. **Stemming**

> Reduce words to their root **form** by chopping off suffixes.

```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem("running")  # â†’ "run"
```

- Aggressive, fast, can be inaccurate.

---

##### ðŸŒ± 6. **Lemmatization**

> More intelligent than stemming. Converts word to **base dictionary form** using grammar rules.

```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize("running", pos='v')  # â†’ "run"
```

- Slower but more accurate than stemming.

---

##### ðŸŽ­ 7. **Part of Speech (POS) Tagging**

> Assign grammatical roles like noun, verb, adjective, etc.

```python
from nltk import pos_tag
pos_tag(['I', 'am', 'learning', 'NLP'])
# â†’ [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]
```

- Helps improve lemmatization and entity recognition.

---

##### ðŸ§  8. **Named Entity Recognition (NER)** _(optional at preprocessing stage)_

> Extract names, dates, places, organizations.

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Elon Musk founded SpaceX in California.")
[(ent.text, ent.label_) for ent in doc.ents]
```

---

##### ðŸ§¹ 9. **Removing Rare Words or Frequent Words**

> Based on frequency distribution, remove:

- Words that appear _too often_ (e.g., â€œalsoâ€, â€œjustâ€)
- Words that appear _too rarely_ (might be noise)

---

##### ðŸ”„ 10. **Spelling Correction**

> Fix common typos: â€œrecieveâ€ â†’ â€œreceiveâ€, â€œtehâ€ â†’ â€œtheâ€.

Libraries: `textblob`, `symspellpy`, `autocorrect`

---

##### ðŸ§ª 11. **Contraction Handling**

> Expand: â€œdonâ€™tâ€ â†’ â€œdo notâ€, â€œIâ€™llâ€ â†’ â€œI willâ€

Why? Models may learn â€œdonâ€™tâ€ and â€œdoâ€ separately â€” better to standardize.

---

##### ðŸ”„ 12. **Normalization**

> Bringing similar words to a consistent form:

- â€œuâ€ â†’ â€œyouâ€
- â€œcooolâ€ â†’ â€œcoolâ€
- â€œ10kâ€ â†’ â€œ10000â€

Can include:

- Unicode normalization
- Slang replacement
- Handling elongated words

---

##### ðŸŒ 13. **Text Cleaning Pipelines (Custom Functions)**

> Combine all steps into a function or `sklearn.Pipeline` for repeated use.

---

##### ðŸ§¾ 14. **Text Preprocessing for Deep Learning / Transformers**

> Special steps:

- Padding
- Truncation
- Special tokens (CLS, SEP)
- Attention masks
- Token IDs

Handled automatically by **`transformers`** library, but must understand how they work.

---

##### ðŸ§  BONUS: Multilingual & Domain-Specific Preprocessing

- **Different languages** need different tokenizers, rules (e.g., Chinese, Arabic)
- **Medical/legal/finance texts** often need domain-specific stopwords or vocab.

---

### ðŸ§° Summary Table of Preprocessing Techniques

|Technique|Required For|Tool/Library|
|---|---|---|
|Lowercase|Uniformity|Python|
|Remove Punctuations/Noise|Clean input|`re`, `string`|
|Tokenization|Split into units|`nltk`, `spaCy`, `transformers`|
|Stopwords Removal|Remove unimportant words|`nltk`, `spaCy`|
|Stemming|Reduce to root|`nltk`, `PorterStemmer`|
|Lemmatization|Dictionary form|`WordNet`, `spaCy`|
|POS Tagging|Contextual grammar|`nltk`, `spaCy`|
|NER|Named entity detection|`spaCy`, `transformers`|
|Contractions|Expand|`contractions`, regex|
|Spelling Correction|Typo fixes|`textblob`, `autocorrect`|
|Normalization|Standardization|Custom / `textnormalizer`|
|Transformers Preprocessing|Input to models|`transformers.Tokenizer`|

---
---
