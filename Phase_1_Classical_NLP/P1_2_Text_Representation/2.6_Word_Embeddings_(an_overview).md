# Word Embeddings

## 🚀 What Are Word Embeddings?

Word embeddings are **vector representations** of words, where:

- Words with **similar meanings** are close together in vector space.
- Unlike one-hot or TF-IDF vectors, embeddings are **dense**, low-dimensional, and **capture semantics**.

> 💡 Think of it as giving _meaning_ to words in numbers.

## 🪜 Why Move Beyond TF-IDF/N-grams?

|Feature|TF-IDF / BoW|Word Embeddings|
|---|---|---|
|Semantic similarity|❌ No|✅ Yes|
|Vector size|Vocabulary size|Fixed (e.g., 100–300)|
|Sparse or Dense|Sparse|Dense|
|Learns from corpus context|❌ No|✅ Yes|
|Word ordering/context|❌ Lost|✅ Captured (some models)|

## 📚 Categories of Word Embeddings

### 🔹 1. **Static Embeddings**

Each word has **one vector**, regardless of context.

|Method|Highlights|
|---|---|
|**Word2Vec**|Learns from co-occurrence in local window|
|**GloVe**|Uses global co-occurrence statistics|
|**FastText**|Adds character n-grams — handles rare/OOV words|

→ We’ll cover each in detail soon.

### 🔸 2. **Contextual Embeddings**

Each word's vector **changes based on its sentence**.

|Method|Highlights|
|---|---|
|**ELMo**|BiLSTM-based, captures syntax/semantics using full context|
|**BERT**|Transformer-based, bidirectional, powerful for many tasks|
|**RoBERTa**|Optimized version of BERT|
|**GPT**|Autoregressive transformer, used for generation|

## 🧭 Summary of Popular Methods

|Model|Type|Captures Word Meaning?|Handles Polysemy?|Architecture Used|
|---|---|---|---|---|
|Word2Vec|Static|✅ Yes|❌ No|Shallow NN|
|GloVe|Static|✅ Yes|❌ No|Matrix Factorization|
|FastText|Static+Subword|✅ Better|❌ No|n-grams|
|ELMo|Contextual|✅ Yes|✅ Yes|BiLSTM|
|BERT|Contextual|✅ Yes|✅ Yes|Transformer|
|GPT|Contextual|✅ Yes|✅ Yes|Transformer (decoder)|

## 🧑‍🏫 Real-world Example

> Consider the word **"bank"**:

|Sentence|Meaning|TF-IDF/Word2Vec|BERT|
|---|---|---|---|
|“He went to the **bank** to fish”|River bank|❌ Same vector|✅ Different vector|
|“He deposited cash at the **bank**”|Financial bank|❌ Same vector|✅ Different vector|

Only **contextual embeddings** (like BERT) distinguish them.

## ⚙️ Applications of Embeddings

- Text Classification
- Clustering / Topic Modeling
- Sentiment Analysis
- Named Entity Recognition
- Machine Translation
- Question Answering
- Search/Recommendation

## 🔧 Tools and Libraries

|Task|Tool / Function|
|---|---|
|Load Word2Vec|`gensim.models.Word2Vec`|
|Load GloVe|Load pre-trained vectors|
|Use FastText|`fasttext`, `gensim.models.FastText`|
|Use BERT|`transformers` library (HuggingFace)|
|Dimensionality Reduction|`TSNE`, `PCA` for visualization|

## 🧭 What We’ll Learn in Detail Next

We’ll dive into each method like this:

### 🔹 A. **Word2Vec**

- CBOW vs Skip-gram
- Training mechanism (negative sampling, hierarchical softmax)
- Code: training, loading pre-trained
- Visualizing embeddings

### 🔹 B. **GloVe**

- Matrix factorization
- How it captures global statistics
- Code to use pre-trained GloVe vectors

### 🔹 C. **FastText**

- Subword modeling
- Handling OOV words
- Code to train & use

### 🔸 D. **ELMo, BERT, etc.**

- Architecture overview
- Contextual nature
- Using with HuggingFace
- Tokenization & embeddings extraction

## ✅ Summary

|Static Models|Word2Vec, GloVe, FastText|
|---|---|
|Contextual Models|ELMo, BERT, RoBERTa, GPT|
|Useful for|Semantic meaning & downstream tasks|
