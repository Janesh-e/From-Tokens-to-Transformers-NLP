# Word Embeddings

## ğŸš€ What Are Word Embeddings?

Word embeddings are **vector representations** of words, where:

- Words with **similar meanings** are close together in vector space.
- Unlike one-hot or TF-IDF vectors, embeddings are **dense**, low-dimensional, and **capture semantics**.

> ğŸ’¡ Think of it as giving _meaning_ to words in numbers.

## ğŸªœ Why Move Beyond TF-IDF/N-grams?

|Feature|TF-IDF / BoW|Word Embeddings|
|---|---|---|
|Semantic similarity|âŒ No|âœ… Yes|
|Vector size|Vocabulary size|Fixed (e.g., 100â€“300)|
|Sparse or Dense|Sparse|Dense|
|Learns from corpus context|âŒ No|âœ… Yes|
|Word ordering/context|âŒ Lost|âœ… Captured (some models)|

## ğŸ“š Categories of Word Embeddings

### ğŸ”¹ 1. **Static Embeddings**

Each word has **one vector**, regardless of context.

|Method|Highlights|
|---|---|
|**Word2Vec**|Learns from co-occurrence in local window|
|**GloVe**|Uses global co-occurrence statistics|
|**FastText**|Adds character n-grams â€” handles rare/OOV words|

â†’ Weâ€™ll cover each in detail soon.

### ğŸ”¸ 2. **Contextual Embeddings**

Each word's vector **changes based on its sentence**.

|Method|Highlights|
|---|---|
|**ELMo**|BiLSTM-based, captures syntax/semantics using full context|
|**BERT**|Transformer-based, bidirectional, powerful for many tasks|
|**RoBERTa**|Optimized version of BERT|
|**GPT**|Autoregressive transformer, used for generation|

## ğŸ§­ Summary of Popular Methods

|Model|Type|Captures Word Meaning?|Handles Polysemy?|Architecture Used|
|---|---|---|---|---|
|Word2Vec|Static|âœ… Yes|âŒ No|Shallow NN|
|GloVe|Static|âœ… Yes|âŒ No|Matrix Factorization|
|FastText|Static+Subword|âœ… Better|âŒ No|n-grams|
|ELMo|Contextual|âœ… Yes|âœ… Yes|BiLSTM|
|BERT|Contextual|âœ… Yes|âœ… Yes|Transformer|
|GPT|Contextual|âœ… Yes|âœ… Yes|Transformer (decoder)|

## ğŸ§‘â€ğŸ« Real-world Example

> Consider the word **"bank"**:

|Sentence|Meaning|TF-IDF/Word2Vec|BERT|
|---|---|---|---|
|â€œHe went to the **bank** to fishâ€|River bank|âŒ Same vector|âœ… Different vector|
|â€œHe deposited cash at the **bank**â€|Financial bank|âŒ Same vector|âœ… Different vector|

Only **contextual embeddings** (like BERT) distinguish them.

## âš™ï¸ Applications of Embeddings

- Text Classification
- Clustering / Topic Modeling
- Sentiment Analysis
- Named Entity Recognition
- Machine Translation
- Question Answering
- Search/Recommendation

## ğŸ”§ Tools and Libraries

|Task|Tool / Function|
|---|---|
|Load Word2Vec|`gensim.models.Word2Vec`|
|Load GloVe|Load pre-trained vectors|
|Use FastText|`fasttext`, `gensim.models.FastText`|
|Use BERT|`transformers` library (HuggingFace)|
|Dimensionality Reduction|`TSNE`, `PCA` for visualization|

## ğŸ§­ What Weâ€™ll Learn in Detail Next

Weâ€™ll dive into each method like this:

### ğŸ”¹ A. **Word2Vec**

- CBOW vs Skip-gram
- Training mechanism (negative sampling, hierarchical softmax)
- Code: training, loading pre-trained
- Visualizing embeddings

### ğŸ”¹ B. **GloVe**

- Matrix factorization
- How it captures global statistics
- Code to use pre-trained GloVe vectors

### ğŸ”¹ C. **FastText**

- Subword modeling
- Handling OOV words
- Code to train & use

### ğŸ”¸ D. **ELMo, BERT, etc.**

- Architecture overview
- Contextual nature
- Using with HuggingFace
- Tokenization & embeddings extraction

## âœ… Summary

|Static Models|Word2Vec, GloVe, FastText|
|---|---|
|Contextual Models|ELMo, BERT, RoBERTa, GPT|
|Useful for|Semantic meaning & downstream tasks|
