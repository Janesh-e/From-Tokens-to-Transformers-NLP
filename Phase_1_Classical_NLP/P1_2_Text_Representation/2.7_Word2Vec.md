# ğŸ” Word2Vec

## What is Word2Vec?

**Word2Vec** is a neural network-based technique that learns **vector representations (embeddings)** of words such that:

- **Similar words** appear close together in the vector space.
- Captures **semantic** and **syntactic** relationships.

For example:

- `vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")` âœ”ï¸

---

## Two Architectures

Word2Vec has two training models:

|Model|Predicts|Good For|
|---|---|---|
|**CBOW**|Center word from context|Faster, small datasets|
|**Skip-gram**|Context from center word|Better for rare words / large corpus|

### Continuous Bag of Words (CBOW)

Predict the target (center) word using the surrounding **context words**.

> Input: `["I", "to", "NLP"]` â†’ Predict: `"love"`

### Skip-Gram

Do the opposite: use the center word to predict its **context**.

> Input: `"love"` â†’ Predict: `["I", "to", "NLP"]`

---

## How Word2Vec Learns?

Each model learns **weights** that act as embeddings using a **shallow neural network** with:

- Input layer: One-hot vector of word
- Hidden layer: Embedding layer (size = 100â€“300)
- Output layer: Softmax over vocabulary

But since the vocabulary is huge, it uses:

- **Negative Sampling** or
- **Hierarchical Softmax** to make it efficient

### ğŸ”· What is Word2Vec Doing?

Word2Vec is **not** trying to classify anything or do language generation.  
Its goal is to **learn embeddings** â€” i.e., assign a unique vector (set of numbers) to each word such that:

> â€œWords that appear in similar contexts get similar vectors.â€

To do that, it _pretends_ itâ€™s doing a classification task â€” and uses that setup to **update word vectors** through learning.

### ğŸ§  Think of Word2Vec as a Shallow Neural Network

### Structure:

```
         (Input word: 'NLP')
           [0, 0, 1, 0, ..., 0]         <- One-hot vector
                   |
          [Input Layer]
                   |
          [Hidden Layer]
       (no activation function)
     (Weights are the embeddings we learn)
                   |
         [Output Layer]
    (Softmax over entire vocabulary)
                   |
       (Predict context words)
```

### ğŸ”¶ Why One-Hot at Input?

Letâ€™s say you have a vocabulary of 10,000 words. The input is a **one-hot vector**:

- Only 1 position is `1` (for the word you care about), rest are 0s.

So, for â€œNLPâ€ (letâ€™s say index = 2), the input looks like:

```plaintext
[0, 0, 1, 0, 0, ..., 0] â†’ Size = 10,000
```

### ğŸ”¶ Hidden Layer â€” Where Magic Happens

Thereâ€™s **no activation** (linear transformation only).  
So weâ€™re essentially doing:

```
Embedding vector = Input vector Ã— Weight matrix
```

This weight matrix has:

- Shape: `[vocab_size x embedding_dim]`
- Example: `[10,000 x 300]`

ğŸ’¡ So when you input a one-hot vector, it **selects one row** from this matrix â€” that becomes your wordâ€™s embedding.

### ğŸ”¶ Output Layer â€” Softmax Over Vocabulary

Letâ€™s say weâ€™re doing **Skip-gram**, where input = â€œNLPâ€  
We want the network to predict the context: `["I", "love", "and"]`

This means: The network outputs a **probability distribution** over all words in the vocabulary â€” and tries to assign **high probabilities** to the correct context words.

This is done using **Softmax**:

```python
Softmax(z) = exp(z_i) / sum(exp(z_j))  for all j in vocab
```

âš ï¸ But hereâ€™s the **big problem**:

- Vocabulary is **huge** (100,000 words)
- Softmax has to compute `exp()` for every single word
- Too slow and memory-heavy

## The Efficiency Problem

So how do we avoid calculating the full softmax over 100,000 words?

## âœ… The Solution: **Negative Sampling** (or **Hierarchical Softmax**)

### ğŸ”» 1. Negative Sampling (most popular)

Instead of predicting over the **whole vocabulary**, we say:

> â€œLetâ€™s just update the weights for the actual correct word, and a few (say 5â€“20) **random wrong words**.â€

These wrong words are called **negative samples**.

ğŸ› ï¸ In training:

- Positive sample: actual (center, context) word pair
- Negative samples: randomly chosen word pairs that _donâ€™t_ co-occur

Then we use a small binary classifier to say:

- Should word A and word B appear together? â†’ Yes/No

This makes training **way faster** while still giving **good quality embeddings**.


### Intuition:

|Skip-gram wants to learn:|
|---|
|â€œGiven word = NLP, context = [love, I, learning]â€ â€” are these likely?â€|
|Use dot product between vectors â†’ sigmoid â†’ binary classification|
|Update embeddings only for those words involved|

### ğŸ”º 2. Hierarchical Softmax

Alternative to negative sampling:

- Builds a **binary tree** where each word is a **leaf**
- Predicts the path from root to word
- Time complexity: `O(log(V))` instead of `O(V)`

Less common, more complex â€” we can skip this for now.

### ğŸ“Œ Summary

|Term|Meaning|
|---|---|
|One-hot vector|Input where only one word is '1', others are '0'|
|Hidden layer|Embedding matrix where each word gets its dense vector (300D etc.)|
|Softmax|Converts raw outputs into probability distribution over vocab|
|Negative sampling|Efficient way to train â€” update only a few words per training step|
|Embedding|The actual vectors you get from trained model, one per word|

---

## ğŸ”§ Word2Vec â€“ Coding with `gensim`

Let's try a Skip-Gram model on a toy corpus.

### Step 1: Install & Import

```bash
pip install gensim
```

```python
from gensim.models import Word2Vec
```

### Step 2: Prepare Corpus

```python
sentences = [
    ['i', 'love', 'nlp'],
    ['nlp', 'is', 'fun'],
    ['i', 'enjoy', 'machine', 'learning'],
    ['deep', 'learning', 'is', 'part', 'of', 'ai'],
    ['nlp', 'uses', 'machine', 'learning']
]
```

### Step 3: Train Word2Vec

```python
model = Word2Vec(
    sentences=sentences,
    vector_size=100,     # embedding dimension
    window=2,            # context window size
    min_count=1,         # ignore words below this freq
    sg=1,                # 1 = skip-gram; 0 = CBOW
    workers=2,           # parallel threads
    epochs=100           # training epochs
)
```

### Step 4: Explore the Model

```python
# Vector for a word
vector = model.wv['nlp']
print(vector[:10])  # first 10 dims

# Most similar words
print(model.wv.most_similar('nlp'))

# Cosine similarity
print(model.wv.similarity('nlp', 'learning'))
```

### Saving & Loading Model

```python
model.save("word2vec_model.model")
model = Word2Vec.load("word2vec_model.model")
```

---

## Visualizing Word Embeddings

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

words = list(model.wv.index_to_key)
vectors = [model.wv[word] for word in words]

pca = PCA(n_components=2)
result = pca.fit_transform(vectors)

plt.figure(figsize=(8,6))
plt.scatter(result[:, 0], result[:, 1])

for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]))

plt.title("Word Embeddings PCA")
plt.grid()
plt.show()
```

---

## How Word2Vec Captures Meaning?

Thanks to **co-occurrence** and **dense training**:

- â€œkingâ€, â€œqueenâ€, â€œprinceâ€, â€œmonarchâ€ cluster together.
- â€œnlpâ€, â€œmachineâ€, â€œdeepâ€ will be close in vector space if trained on good corpus.

---

## âš ï¸ Limitations

|Issue|Explanation|
|---|---|
|No context awareness|"bank" means same in both â€œriver bankâ€ & â€œmoney bankâ€|
|Fixed vocabulary|Cannot handle new (OOV) words|
|Sentence-level meaning lost|Each word is isolated|

---

## ğŸ“š Best Practices

- Use **Skip-gram** if you have large corpus and care about rare words.
- Use **CBOW** if your corpus is small or you want faster training.
- Use pre-trained Word2Vec if training is not feasible:
    - [`GoogleNews-vectors-negative300`](https://code.google.com/archive/p/word2vec/)

```python
# Load pre-trained binary Google Word2Vec
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
```

---

## ğŸ§­ Whatâ€™s Next?

Weâ€™ll now move to:
-  **GloVe**: Uses co-occurrence matrix + matrix factorization.
-  **FastText**: Uses subword info to handle rare/new words.
