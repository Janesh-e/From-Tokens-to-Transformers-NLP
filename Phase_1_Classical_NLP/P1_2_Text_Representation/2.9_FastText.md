# FastText

**FastText**, developed by Facebook AI Research (FAIR), is an extension of Word2Vec that represents a word as a **bag of character n-grams**.

 It takes the best parts of Word2Vec and **goes a step further** by breaking words into **subword units** (character n-grams). This allows FastText to:

- Handle rare words better
- Understand out-of-vocabulary (OOV) words
- Deal with misspellings and morphologically rich languages

For example:

- `"apple"` with 3-grams becomes:  
    `"<ap", "app", "ppl", "ple", "le>"`  
    (with `<` and `>` marking the beginning and end)

Instead of learning a vector for the word `"apple"`, it learns vectors for each subword (like `"app"`, `"ppl"`), and the final word vector is the **sum or average of its subword vectors**.

---

## Why is FastText Powerful?

|Word2Vec|FastText|
|---|---|
|Learns one vector per word|Learns vectors for word _and_ its subwords|
|Doesn‚Äôt handle OOV words|Can generate vectors for unseen words|
|No info on morphology|Understands suffixes, prefixes, and roots|
|No spelling mistake support|Can infer meaning of typos like "machin"|

---

## Internal Working of FastText

### Input Representation

Let‚Äôs say you're using **3‚Äì6 character n-grams**.

Word: `"playing"`  
N-grams:

- `<pl`, `pla`, `lay`, `ayi`, `yin`, `ing`, `ng>`, `...`

Each of these **subwords** gets a vector during training.

So:

```plaintext
Vector("playing") = average( all n-gram vectors + full word vector )
```

### Training Objective

FastText uses **Skip-gram** (like Word2Vec), but instead of predicting context using the word vector, it uses the **sum of subword vectors**:

```python
input = average([vec("pla"), vec("lay"), vec("playing")])
```

Then predict the surrounding context words with **negative sampling** or **hierarchical softmax**.

### üí° Why This Helps:

- If `"biology"` and `"biologist"` share `"bio"` and `"logy"` ‚Äî they will get similar embeddings.
- Even a misspelled word like `"enginering"` will share enough n-grams with `"engineering"` to make its vector useful.

---

## üíª Coding with FastText ‚Äì Two Ways

### üî∏ Option 1: Use Pretrained FastText from Gensim

```bash
pip install gensim
```

```python
from gensim.models import FastText

sentences = [
    ["i", "love", "nlp"],
    ["deep", "learning", "is", "fun"],
    ["nlp", "uses", "machine", "learning"]
]

# Train model
model = FastText(sentences, vector_size=100, window=3, min_count=1, epochs=10)

# Get word vector
print(model.wv["learning"])

# Works for OOV too!
print(model.wv["learniing"])  # Typo!
```

### üî∏ Option 2: Use Facebook‚Äôs FastText Library (Command-line)

```bash
pip install fasttext
```

```python
import fasttext

# Train on unsupervised text file
model = fasttext.train_unsupervised("corpus.txt", model='skipgram')

# Get vector
print(model.get_word_vector("biology"))

# Similar words
print(model.get_nearest_neighbors("biology"))
```

### ‚ö†Ô∏è Caution:

FastText's `.bin` models from Facebook can't be used directly with Gensim ‚Äî you'll need converters or use the Facebook Python package directly.

---

## üìÅ Pretrained FastText Embeddings

Download from [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)

```plaintext
cc.en.300.bin
cc.en.300.vec
```

Use `.vec` with Gensim, `.bin` with Facebook‚Äôs `fasttext` library.

---

## Analogy & Similarity Examples

```python
model.wv.most_similar("queen")
# [('princess', 0.84), ('monarch', 0.82), ...]

model.wv.most_similar(positive=['king', 'woman'], negative=['man'])
# [('queen', 0.89), ('duchess', 0.81), ...]
```

---

## Comparison Table

|Feature|Word2Vec|GloVe|FastText|
|---|---|---|---|
|Subword info|‚ùå|‚ùå|‚úÖ|
|OOV handling|‚ùå|‚ùå|‚úÖ|
|Typo tolerance|‚ùå|‚ùå|‚úÖ|
|Training method|Skip-gram/CBOW|Matrix factorization|Skip-gram + subword bag|
|Speed|Fast|Slower|Slightly slower than Word2Vec|
|Accuracy|Good|Good|Often better for rare words|

---

## Summary

- FastText improves Word2Vec by breaking each word into character-level n-grams.
- This allows it to capture **morphology**, **prefix/suffix meaning**, & **handle rare/misspelled words**.
- Word vector = sum of subword vectors
- Uses Skip-gram + negative sampling under the hood

---
