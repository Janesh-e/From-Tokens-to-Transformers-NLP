# ðŸ“¦ TEXT REPRESENTATION in NLP â€” Introduction 

## ðŸ” What is Text Representation?

> **Text Representation** is the process of **converting textual data (words, sentences, documents)** into **numerical form (vectors)** so that machine learning models can understand and process it.

ðŸ’¡ Machine learning algorithms canâ€™t work with raw text â€” they need **numbers**.

## ðŸ§± Why Is Text Representation So Important?

|Purpose|How It Helps|
|---|---|
|Converts language to math|Needed for all ML/DL models|
|Captures meaning & relationships|Similar words â†’ similar vectors|
|Enables downstream tasks|Classification, translation, summarization, etc.|

## ðŸ§­ Types of Text Representations

Here's a structured map of the different approaches we'll cover â€” moving from **simplest** to **most powerful**.

### ðŸ”¹ 1. **One-Hot Encoding**

- Binary vector representing word presence in a vocabulary.
- Doesnâ€™t capture meaning or similarity.
- Large and sparse.

```python
["apple", "banana", "grape"]
â†’ "banana" = [0, 1, 0]
```

### ðŸ”¹ 2. **Bag of Words (BoW)**

- Counts how many times each word appears in a document.
- Ignores grammar and word order.
- Still sparse but more informative.

```python
doc1: "I love NLP"
doc2: "NLP is great"
BoW â†’ {"I":1, "love":1, "NLP":2, "is":1, "great":1}
```

### ðŸ”¹ 3. **TF-IDF (Term Frequency-Inverse Document Frequency)**

- Highlights important words by balancing frequency in a document vs across all documents.
- Still sparse and doesnâ€™t capture meaning, but better than BoW.

```python
TF: Frequency of "NLP" in a doc  
IDF: Inverse of how common "NLP" is across docs  
TF-IDF = TF Ã— IDF
```

### ðŸ”¹ 4. **N-grams**

- Represents sequences of words.
- Preserves word order in small chunks.
- Useful for capturing short context like "New York", "not good".

```python
"Natural Language Processing"
Bigrams: ["Natural Language", "Language Processing"]
```

### ðŸ”¹ 5. **Word Embeddings (Dense Vector Representations)**

These **dense, low-dimensional** representations learn the meaning of words from context using deep learning.

|Model|Description|
|---|---|
|**Word2Vec**|Learns word meaning from context using skip-gram/CBOW|
|**GloVe**|Learns from global word-word co-occurrence matrix|
|**FastText**|Includes subword information, better for rare words|
|**ELMo**|Contextual embeddings using LSTMs (word meaning changes with context)|

### ðŸ”¹ 6. **Contextual Embeddings (LLM-based)**

These are **state-of-the-art**, deep contextualized embeddings. The meaning of a word depends on the **sentence it appears in**.

|Model|Description|
|---|---|
|**BERT**|Transformer-based, bi-directional context|
|**RoBERTa**|Improved BERT with more training|
|**DistilBERT**|Smaller, faster version of BERT|
|**GPT Embeddings**|Word vectors from autoregressive models like GPT|
|**Sentence Transformers**|Encode full sentences, useful for similarity/search|

## ðŸ“Š Comparison Table

| Method            | Captures Semantics? | Considers Order? | Dense? | Learns Context? |
| ----------------- | ------------------- | ---------------- | ------ | --------------- |
| One-Hot           | âŒ                   | âŒ                | âŒ      | âŒ               |
| BoW               | âŒ                   | âŒ                | âŒ      | âŒ               |
| TF-IDF            | âŒ (partially)       | âŒ                | âŒ      | âŒ               |
| N-grams           | âœ… (limited)         | âœ… (short range)  | âŒ      | âŒ               |
| Word2Vec          | âœ…                   | âŒ                | âœ…      | âŒ               |
| FastText          | âœ…                   | âŒ                | âœ…      | âŒ               |
| ELMo              | âœ…                   | âœ…                | âœ…      | âœ…               |
| BERT/Transformers | âœ…                   | âœ…                | âœ…      | âœ…               |

## ðŸ§  When to Use What?

|Task|Suggested Representation|
|---|---|
|Small/simple models|TF-IDF or BoW|
|Sentiment/classification|Word2Vec / FastText / BERT|
|Search / recommendation|Sentence Transformers / BERT|
|Chatbots / QA systems|BERT / GPT embeddings|
|Similarity/Clustering|Word2Vec / Sentence Embeddings|
|Real-time inference|DistilBERT / FastText|

## ðŸ§ª Libraries Youâ€™ll Use for Each

|Technique|Libraries|
|---|---|
|One-hot, BoW|`scikit-learn`, `nltk`|
|TF-IDF|`scikit-learn`|
|Word2Vec|`gensim`|
|FastText|`gensim`, `facebookresearch/fastText`|
|GloVe|Pre-trained vectors (use with `gensim` or load directly)|
|BERT/LLMs|`transformers`, `sentence-transformers`, `spacy`, `torch`|

## ðŸ”š Summary

> **Text Representation** bridges the gap between human language and machine learning by encoding text into meaningful numbers.

It evolves through **increasing levels of intelligence**:

```
Tokens â†’ Frequencies â†’ Meaning â†’ Contextual Meaning
```
