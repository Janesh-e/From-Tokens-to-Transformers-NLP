# 📦 TEXT REPRESENTATION in NLP — Introduction 

## 🔍 What is Text Representation?

> **Text Representation** is the process of **converting textual data (words, sentences, documents)** into **numerical form (vectors)** so that machine learning models can understand and process it.

💡 Machine learning algorithms can’t work with raw text — they need **numbers**.

## 🧱 Why Is Text Representation So Important?

|Purpose|How It Helps|
|---|---|
|Converts language to math|Needed for all ML/DL models|
|Captures meaning & relationships|Similar words → similar vectors|
|Enables downstream tasks|Classification, translation, summarization, etc.|

## 🧭 Types of Text Representations

Here's a structured map of the different approaches we'll cover — moving from **simplest** to **most powerful**.

### 🔹 1. **One-Hot Encoding**

- Binary vector representing word presence in a vocabulary.
- Doesn’t capture meaning or similarity.
- Large and sparse.

```python
["apple", "banana", "grape"]
→ "banana" = [0, 1, 0]
```

### 🔹 2. **Bag of Words (BoW)**

- Counts how many times each word appears in a document.
- Ignores grammar and word order.
- Still sparse but more informative.

```python
doc1: "I love NLP"
doc2: "NLP is great"
BoW → {"I":1, "love":1, "NLP":2, "is":1, "great":1}
```

### 🔹 3. **TF-IDF (Term Frequency-Inverse Document Frequency)**

- Highlights important words by balancing frequency in a document vs across all documents.
- Still sparse and doesn’t capture meaning, but better than BoW.

```python
TF: Frequency of "NLP" in a doc  
IDF: Inverse of how common "NLP" is across docs  
TF-IDF = TF × IDF
```

### 🔹 4. **N-grams**

- Represents sequences of words.
- Preserves word order in small chunks.
- Useful for capturing short context like "New York", "not good".

```python
"Natural Language Processing"
Bigrams: ["Natural Language", "Language Processing"]
```

### 🔹 5. **Word Embeddings (Dense Vector Representations)**

These **dense, low-dimensional** representations learn the meaning of words from context using deep learning.

|Model|Description|
|---|---|
|**Word2Vec**|Learns word meaning from context using skip-gram/CBOW|
|**GloVe**|Learns from global word-word co-occurrence matrix|
|**FastText**|Includes subword information, better for rare words|
|**ELMo**|Contextual embeddings using LSTMs (word meaning changes with context)|

### 🔹 6. **Contextual Embeddings (LLM-based)**

These are **state-of-the-art**, deep contextualized embeddings. The meaning of a word depends on the **sentence it appears in**.

|Model|Description|
|---|---|
|**BERT**|Transformer-based, bi-directional context|
|**RoBERTa**|Improved BERT with more training|
|**DistilBERT**|Smaller, faster version of BERT|
|**GPT Embeddings**|Word vectors from autoregressive models like GPT|
|**Sentence Transformers**|Encode full sentences, useful for similarity/search|

## 📊 Comparison Table

| Method            | Captures Semantics? | Considers Order? | Dense? | Learns Context? |
| ----------------- | ------------------- | ---------------- | ------ | --------------- |
| One-Hot           | ❌                   | ❌                | ❌      | ❌               |
| BoW               | ❌                   | ❌                | ❌      | ❌               |
| TF-IDF            | ❌ (partially)       | ❌                | ❌      | ❌               |
| N-grams           | ✅ (limited)         | ✅ (short range)  | ❌      | ❌               |
| Word2Vec          | ✅                   | ❌                | ✅      | ❌               |
| FastText          | ✅                   | ❌                | ✅      | ❌               |
| ELMo              | ✅                   | ✅                | ✅      | ✅               |
| BERT/Transformers | ✅                   | ✅                | ✅      | ✅               |

## 🧠 When to Use What?

|Task|Suggested Representation|
|---|---|
|Small/simple models|TF-IDF or BoW|
|Sentiment/classification|Word2Vec / FastText / BERT|
|Search / recommendation|Sentence Transformers / BERT|
|Chatbots / QA systems|BERT / GPT embeddings|
|Similarity/Clustering|Word2Vec / Sentence Embeddings|
|Real-time inference|DistilBERT / FastText|

## 🧪 Libraries You’ll Use for Each

|Technique|Libraries|
|---|---|
|One-hot, BoW|`scikit-learn`, `nltk`|
|TF-IDF|`scikit-learn`|
|Word2Vec|`gensim`|
|FastText|`gensim`, `facebookresearch/fastText`|
|GloVe|Pre-trained vectors (use with `gensim` or load directly)|
|BERT/LLMs|`transformers`, `sentence-transformers`, `spacy`, `torch`|

## 🔚 Summary

> **Text Representation** bridges the gap between human language and machine learning by encoding text into meaningful numbers.

It evolves through **increasing levels of intelligence**:

```
Tokens → Frequencies → Meaning → Contextual Meaning
```
