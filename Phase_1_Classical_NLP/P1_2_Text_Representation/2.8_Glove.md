# GloVe

**GloVe** stands for **Global Vectors** for word representation.  
It is an unsupervised learning algorithm that learns **dense word embeddings** by:

- Counting **how often words co-occur** with each other
- Learning word vectors such that the **ratios of co-occurrence probabilities** encode semantic meaning.

It is a method that combines the **global statistics of a corpus** (like matrix factorization) with **local context** (like Word2Vec). It was developed by Stanford and is often seen as a middle ground between **Word2Vec** and **LSA**.

---

## Intuition Behind GloVe

> If ‚Äúice‚Äù co-occurs with ‚Äúsolid‚Äù, ‚Äúcold‚Äù a lot, and ‚Äústeam‚Äù co-occurs more with ‚Äúhot‚Äù, ‚Äúgas‚Äù,  
> ... and both co-occur similarly with ‚Äúwater‚Äù ‚Äî  
> Then the **difference between ‚Äúice‚Äù and ‚Äústeam‚Äù vectors** should capture the **meaning**.

So instead of relying on prediction (like Word2Vec), GloVe builds a **co-occurrence matrix** and applies a special cost function to learn word vectors.

## Step-by-Step Working of GloVe

### 1. Build the Co-occurrence Matrix

- Go through a huge corpus.
- For every word pair `(i, j)` within a **window** (say size=5), count how often they appear together.

This gives you a matrix **X**:

|       | king | queen | man | woman | crown |
| ----- | ---- | ----- | --- | ----- | ----- |
| king  | -    | 20    | 35  | 15    | 50    |
| queen | 20   | -     | 10  | 30    | 45    |
| man   | 35   | 10    | -   | 5     | 20    |
| ...   | ...  | ...   | ... | ...   | ...   |

So:  
`X_ij = # of times word j appears in the context of word i`

---

### 2. GloVe's Objective

The **goal** is to learn word vectors `w_i` and context vectors `wÃÉ_j` such that:

$dot(w_i, wÃÉ_j) + b_i + bÃÉ_j ‚âà log(X_ij)$

Then the **loss function** is:

$J = Œ£_ij f(X_ij) * (dot(w_i, wÃÉ_j) + b_i + bÃÉ_j - log(X_ij))^2$

- The model tries to minimize the difference between the **dot product of vectors** and the **log of their co-occurrence**.
- `f(X_ij)` is a **weighting function** to reduce the impact of very frequent word pairs.

### 3. What‚Äôs Actually Learned?

- Each word has:
    - A **word vector** (w)
    - A **context vector** (wÃÉ)
- After training, you **sum or average** them:

```python
final_vector = w + wÃÉ
```

And that‚Äôs the final **embedding** you use.

### Why is GloVe Different?

|Feature|Word2Vec|GloVe|
|---|---|---|
|Approach|Predict context using NN|Count-based matrix factorization|
|Uses frequencies|‚ùå|‚úÖ|
|Local/Global info|Local (sliding window)|Global (entire corpus matrix)|
|Learning method|SGD with negative sampling|Least squares on co-occurrence matrix|
|Training time|Fast, streamable|Slower (matrix + training needed)|

## üõ†Ô∏è Implementing GloVe (Using `glove-python-binary`)

### 1. Install GloVe library

```bash
pip install glove-python-binary
```

### 2. Prepare the Corpus

```python
from glove import Corpus, Glove

# Input corpus
sentences = [
    ["i", "love", "nlp"],
    ["nlp", "is", "fun"],
    ["i", "enjoy", "machine", "learning"],
    ["deep", "learning", "is", "part", "of", "ai"],
    ["nlp", "uses", "machine", "learning"]
]

# Build co-occurrence matrix
corpus = Corpus()
corpus.fit(sentences, window=2)
```

### 3. Train GloVe Model

```python
glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=30, no_threads=2, verbose=True)
glove.add_dictionary(corpus.dictionary)
```

### 4. Use the Vectors

```python
# Find vector for a word
print(glove.word_vectors[glove.dictionary['nlp']])

# Similar words
print(glove.most_similar("nlp", number=3))
```

## Visualizing the Vectors

Same as with Word2Vec: use `PCA` or `t-SNE` for 2D plotting.

## What Makes GloVe Powerful?

- It‚Äôs **not learning by prediction** like Word2Vec.
- It‚Äôs using **global word co-occurrence statistics** to capture relationships between words.
- GloVe captures analogies like:

```python
vec("king") - vec("man") + vec("woman") ‚âà vec("queen")
```

Because word ratios like:

```
P(ice|solid) / P(steam|solid)
```

are **informative** about their meaning.

## üìÅ Pretrained GloVe Embeddings

Available from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)

Examples:
- glove.6B.100d.txt (Wikipedia)
- glove.42B.300d.txt (Common Crawl)

You can load them using Gensim:

```python
from gensim.models.keyedvectors import KeyedVectors
glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False, no_header=True)
```

## Summary

|Component|Explanation|
|---|---|
|Co-occurrence Matrix|Counts how often words appear together|
|Loss Function|Makes dot product of vectors ‚âà log of co-occurrence|
|Weighting Function|Reduces impact of very frequent word pairs|
|Final Embeddings|Sum of word and context vectors|
|Strength|Captures global statistics and semantic relationships|
