# üî¢ One-Hot Encoding (OHE)

## üß† What is One-Hot Encoding?

**One-Hot Encoding** is a way of representing **each word as a binary vector** in which:

- Only **one element is "hot" (1)** ‚Äî the rest are 0.
- The **position** of the 1 indicates which word it is in the vocabulary.

## üì¶ Why Use One-Hot Encoding?

|Strength|Weakness|
|---|---|
|Simple & easy to implement|Doesn‚Äôt capture word meaning|
|Works well for toy datasets|High-dimensional & sparse|
|No training required|No info about word similarity|

## üßæ Example: Let‚Äôs Walk Through It

### üìù Sample corpus:

```python
corpus = ["I love NLP", "NLP is fun"]
```

### Step 1: Build the Vocabulary

List all **unique words** across all documents:

```
Vocabulary = ['I', 'love', 'NLP', 'is', 'fun']
Index Map: {'I':0, 'love':1, 'NLP':2, 'is':3, 'fun':4}
```

Each word will be represented as a 5-length vector (since we have 5 unique words).

## üßë‚Äçüíª CODE: One-Hot Encoding from Scratch

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Step 1: Sample sentences
corpus = ["I love NLP", "NLP is fun"]

# Step 2: Tokenize words (split each sentence into words)
tokens = [sentence.split() for sentence in corpus]
all_words = [word for sentence in tokens for word in sentence]

# Step 3: Create unique vocabulary
vocab = sorted(set(all_words))
print("Vocabulary:", vocab)

# Step 4: Integer encode
word_to_index = {word: idx for idx, word in enumerate(vocab)}
print("\nWord to Index Mapping:")
for word, idx in word_to_index.items():
    print(f"{word}: {idx}")

# Step 5: One-Hot Encode manually
def one_hot_encode(word, vocab_size, word_to_index):
    vec = [0] * vocab_size
    vec[word_to_index[word]] = 1
    return vec

print("\nOne-Hot Encoded Vectors:")
for word in all_words:
    print(f"{word}: {one_hot_encode(word, len(vocab), word_to_index)}")
```

## üñ®Ô∏è OUTPUT (Explained)

### Vocabulary:

```python
['I', 'NLP', 'fun', 'is', 'love']
```

### Word-to-index mapping:

```python
I: 0
NLP: 1
fun: 2
is: 3
love: 4
```

### One-hot encoded vectors:

```python
I:     [1, 0, 0, 0, 0]
love:  [0, 0, 0, 0, 1]
NLP:   [0, 1, 0, 0, 0]
is:    [0, 0, 0, 1, 0]
fun:   [0, 0, 1, 0, 0]
```

Each word is represented by a **vector the size of the vocabulary**, with a `1` at its index.

## üßë‚Äçüî¨ Using `sklearn` OneHotEncoder (for advanced token processing)

If you have many documents and want to encode them for ML directly:

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

words = np.array(all_words).reshape(-1, 1)
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(words)

print("\nSklearn One-Hot Encoding:")
for word, vec in zip(words.flatten(), encoded):
    print(f"{word}: {vec}")
```

## ‚ö†Ô∏è Limitations of One-Hot Encoding

1. **High Dimensionality**: Large vocab = large vectors.
2. **No Context**: "dog" and "puppy" are equally distant.
3. **No Semantics**: Doesn‚Äôt understand word similarity or order.

## üìö Use Cases

|Scenario|Should You Use One-Hot?|
|---|---|
|Small vocabulary tasks (toy models)|‚úÖ Yes|
|Deep learning (RNNs, Transformers)|‚ùå Use embeddings|
|Categorical ML features (non-NLP)|‚úÖ Yes (common in ML)|

## üîÅ Recap

- One-hot encoding = binary vector where one index is hot (1).
- Vocab size = vector length.
- No understanding of semantics or similarity.
- Best for **simple or educational tasks** ‚Äî replaced by **embeddings** in modern NLP.
