# üìä Bag of Words (BoW)

## üß† What is Bag of Words?

The **Bag of Words model** represents text as a **‚Äúbag‚Äù of individual words**, ignoring grammar and word order, but keeping **word frequency**.

> Each document becomes a vector of word counts based on the vocabulary.

## üßæ Key Characteristics

|Feature|Value|
|---|---|
|Captures frequency|‚úÖ Yes|
|Captures word meaning|‚ùå No|
|Captures word order|‚ùå No|
|Sparse|‚úÖ Often|
|Simple & fast|‚úÖ Yes|

## üì¶ Real-World Analogy

Imagine you're analyzing products in shopping carts:

- Cart 1: `["milk", "bread"]`
- Cart 2: `["milk", "bread", "eggs"]`

Instead of their order, you just care about **how many of each item** is there.

## üìù Example: Step-by-Step

### üìÇ Corpus:

```python
corpus = [
    "I love NLP",
    "NLP is fun",
    "I love fun"
]
```

### Step 1: Build Vocabulary

List of all **unique words**:

```python
['I', 'love', 'NLP', 'is', 'fun']
```

Each document becomes a **5-dimensional vector**:

- Each index counts occurrences of the word in the sentence.

### Step 2: Vector Representation

Now represent each document as a **Bag of Words vector**:

|Document|I|love|NLP|is|fun|
|---|---|---|---|---|---|
|"I love NLP"|1|1|1|0|0|
|"NLP is fun"|0|0|1|1|1|
|"I love fun"|1|1|0|0|1|

### Final Output (as list of vectors)

```python
[
    [1, 1, 1, 0, 0],
    [0, 0, 1, 1, 1],
    [1, 1, 0, 0, 1]
]
```


## üßë‚Äçüíª Manual Implementation (With Explanation)

```python
from sklearn.feature_extraction.text import CountVectorizer

# Sample corpus
corpus = [
    "I love NLP",
    "NLP is fun",
    "I love fun"
]

# Initialize vectorizer
vectorizer = CountVectorizer()

# Fit and transform the corpus
X = vectorizer.fit_transform(corpus)

# Get the vocabulary
vocab = vectorizer.get_feature_names_out()
print("Vocabulary:", vocab)

# Convert BoW matrix to array
print("\nBag of Words Matrix:")
print(X.toarray())

# Link documents and vectors
for i, doc_vec in enumerate(X.toarray()):
    print(f"\nDocument {i+1}: \"{corpus[i]}\"")
    for word, count in zip(vocab, doc_vec):
        print(f"{word}: {count}", end=' | ')
```

## üñ®Ô∏è OUTPUT

### Vocabulary (Alphabetically Sorted)

```python
['fun', 'is', 'love', 'nlp']
```

> Note: `CountVectorizer` converts all to lowercase and ignores punctuation by default.

### BoW Matrix:

```
[
 [0, 0, 1, 1],  # "I love NLP" ‚Üí 0 fun, 0 is, 1 love, 1 nlp
 [1, 1, 0, 1],  # "NLP is fun"
 [1, 0, 1, 0]   # "I love fun"
]
```

(Note: The word order in vocabulary may vary depending on the tool used. Our manual example has: ['I', 'love', 'NLP', 'is', 'fun'])

## üîç Understanding Each Row (Vector)

|Document|fun|is|love|nlp|
|---|---|---|---|---|
|"I love NLP"|0|0|1|1|
|"NLP is fun"|1|1|0|1|
|"I love fun"|1|0|1|0|

Each row tells us how many times each word in the vocab appeared in the sentence.

## üß† How it Works Internally

1. **Text Cleaning**: Removes punctuation, converts to lowercase.
2. **Tokenization**: Splits text into words.
3. **Vocabulary Creation**: Builds a set of all unique words in the corpus.
4. **Vectorization**: Creates vectors where each element is the **count** of a vocab word.

## üîß Customization Options with `CountVectorizer`

```python
CountVectorizer(
    lowercase=True,       # Convert to lowercase
    stop_words='english', # Remove common stopwords
    ngram_range=(1,2),    # Include unigrams and bigrams
    max_features=1000     # Limit vocab size
)
```

## Limitations of BoW

|Limitation|Description|
|---|---|
|No word order|‚Äúnot good‚Äù vs ‚Äúgood not‚Äù = same|
|Sparse and high-dimensional|Each new word adds a new dimension|
|No understanding of semantics|‚Äúgreat‚Äù and ‚Äúawesome‚Äù treated as unrelated|
|Affected by synonyms & polysemy|Can‚Äôt tell similar words or word sense|

## When Should You Use BoW?

|Scenario|Should You Use BoW?|
|---|---|
|Small datasets|‚úÖ Yes|
|Quick text classification (baseline)|‚úÖ Yes|
|Deep semantics or LLMs|‚ùå Use embeddings|
|Word order / context-sensitive task|‚ùå Use n-grams/embeddings|

## BONUS: Visualizing BoW with Pandas

```python
import pandas as pd

df = pd.DataFrame(X.toarray(), columns=vocab)
print(df)
```

Gives you a beautiful DataFrame showing frequency per document.
