# TF-IDF (Term Frequency‚ÄìInverse Document Frequency)

## üßæ What is TF-IDF?

**TF-IDF** is a **statistical measure** that evaluates how **important a word is to a document** in a collection or corpus.

Unlike Bag of Words (BoW), which only counts, **TF-IDF reduces the weight of common words** (like "is", "the") and **boosts rare but informative words**.

## üîç Intuition Behind TF-IDF

- **Term Frequency (TF)**: How often a word appears in a document.
- **Inverse Document Frequency (IDF)**: How rare the word is across all documents.

> **TF-IDF = TF √ó IDF**

## üìå Why Use TF-IDF?

|Feature|TF-IDF Benefits|
|---|---|
|Weighted Representation|Emphasizes rare & informative terms|
|Reduces Noise|Downweights common stop words|
|Sparse but Smarter|Like BoW, but more meaningful|
|Fast & Efficient|Works well for ML models|

## üß† Formula Breakdown

### üîπ Term Frequency (TF)

$TF(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}$

### üîπ Inverse Document Frequency (IDF)

$IDF(t) = \log\left(\frac{1 + N}{1 + df(t)}\right) + 1$

- $N$: Total number of documents
- $df(t)$: Number of documents containing term $t$
- **+1 smoothing** is often applied to avoid division by zero.

## Example Corpus

```python
corpus = [
    "I love NLP",
    "NLP is fun",
    "I love fun"
]
```

## üßë‚Äçüíª CODE: TF-IDF using `TfidfVectorizer`

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample corpus
corpus = [
    "I love NLP",
    "NLP is fun",
    "I love fun"
]

# Initialize vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform
X = vectorizer.fit_transform(corpus)

# Feature names (vocabulary)
vocab = vectorizer.get_feature_names_out()
print("Vocabulary:", vocab)

# TF-IDF matrix
print("\nTF-IDF Matrix:")
print(X.toarray())
```

## üñ®Ô∏è Sample Output

Assuming vocabulary is: `['fun', 'is', 'love', 'nlp']`

```
Document 1: "I love NLP"
Vector: [0.        , 0.        , 0.70710678, 0.70710678]

Document 2: "NLP is fun"
Vector: [0.622766 , 0.622766 , 0.        , 0.474898 ]

Document 3: "I love fun"
Vector: [0.622766 , 0.        , 0.622766 , 0.        ]
```

> The values reflect the **importance** of each word in the document **relative to the corpus**.

## Manual Calculation (One Word Example)

Let‚Äôs manually calculate TF-IDF for the word `"love"` in **Document 1**:

- TF(love, Doc1) = 1 / 3 = 0.333
- DF(love) = 2 documents ‚Üí IDF(love) = log(3 / 2) + 1 ‚âà 1.18
- TF-IDF = 0.333 √ó 1.18 ‚âà 0.393

## ‚öôÔ∏è Parameters in `TfidfVectorizer`

```python
TfidfVectorizer(
    stop_words='english',        # remove common stopwords
    max_df=0.85,                 # ignore terms in >85% docs
    min_df=2,                    # include terms in ‚â•2 docs
    ngram_range=(1,2),           # unigrams + bigrams
    max_features=1000            # limit vocab size
)
```

## Visualize with Pandas

```python
import pandas as pd

df = pd.DataFrame(X.toarray(), columns=vocab)
print(df)
```

## Advantages Over BoW

|Feature|BoW|TF-IDF|
|---|---|---|
|Word Importance|‚ùå Treats all same|‚úÖ Weighted|
|Word Frequency|‚úÖ Counts|‚úÖ Uses freq + rarity|
|Common Words Downweight|‚ùå No|‚úÖ Yes|
|Meaningful Vectors|‚ùå Less meaningful|‚úÖ More meaningful|

## Limitations of TF-IDF

|Limitation|Description|
|---|---|
|Ignores semantics|Still doesn‚Äôt know that "smart" ‚âà "intelligent"|
|Ignores order/context|‚Äúnot good‚Äù still looks positive|
|Sparse vectors|High-dimensional like BoW|
|Doesn‚Äôt handle OOV words|New unseen words will be ignored|

## Use Cases

- Text Classification (e.g., spam detection)
- Document Clustering
- Information Retrieval / Search Engines
- Feature Extraction for ML Models

## Summary

|Aspect|TF-IDF|
|---|---|
|Type|Weighted frequency-based|
|Handles semantics|‚ùå No|
|Handles frequency|‚úÖ Yes|
|Use case|Simple ML models, search indexing|
|Dimensionality|Sparse (but better than BoW)|
