# N-Grams

## 🧠 What are N-grams?

**N-grams** are continuous sequences of **N items (usually words or characters)** from a given text or speech.

> It’s how we add a **basic memory of word order** to NLP models that otherwise treat each word independently.

## 📌 Why N-grams?

Most traditional models (like Bag of Words or TF-IDF) treat words as **independent**, missing the _sequence or context_.

N-grams help capture:

- **Phrase structure**
- **Contextual meaning**
- **Co-occurrence patterns**

## Types of N-grams

|N-gram Type|N Value|Example for “I love NLP”|
|---|---|---|
|Unigram|1|[“I”, “love”, “NLP”]|
|Bigram|2|[“I love”, “love NLP”]|
|Trigram|3|[“I love NLP”]|
|4-gram+|≥4|[“I love NLP tasks”, etc.]|

## 📚 Practical Example

```python
sentence = "I love learning NLP"

# Unigrams
['I', 'love', 'learning', 'NLP']

# Bigrams
['I love', 'love learning', 'learning NLP']

# Trigrams
['I love learning', 'love learning NLP']
```

## 🧑‍💻 Code Example with `CountVectorizer`

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "I love NLP",
    "NLP is fun",
    "I love fun"
]

# Bigram Vectorizer
vectorizer = CountVectorizer(ngram_range=(2, 2))  # (min_n, max_n)
X = vectorizer.fit_transform(corpus)

print("Bigram Vocabulary:")
print(vectorizer.get_feature_names_out())

print("\nBigram Matrix:")
print(X.toarray())
```

### 🖨️ Sample Output

**Vocabulary:**

```
['fun is', 'i love', 'love fun', 'love nlp', 'nlp is']
```

**Matrix (rows = docs, cols = bigrams):**

```
[
 [0, 1, 0, 1, 0],   # "I love NLP"
 [1, 0, 0, 0, 1],   # "NLP is fun"
 [0, 0, 1, 0, 0]    # "I love fun"
]
```

## 📈 N-gram Frequency Plot

To extract **frequent n-grams**:

```python
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter

corpus = ["NLP is fun. NLP is powerful. I love NLP"]

vectorizer = CountVectorizer(ngram_range=(2,2))  # Bigrams
X = vectorizer.fit_transform(corpus)

# Sum frequencies
counts = X.sum(axis=0).A1
vocab = vectorizer.get_feature_names_out()

# Create frequency dictionary
freq_dict = dict(zip(vocab, counts))
sorted_ngrams = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)

print("Top N-grams:")
for phrase, freq in sorted_ngrams:
    print(f"{phrase}: {freq}")
```

## 🧠 When to Use N-grams?

|Use Case|N-gram Type|
|---|---|
|Word-based search improvements|Bigrams|
|Detecting phrases & collocations|Bigrams/Trigrams|
|Language modeling (predict next word)|Trigrams or more|
|Spelling/typo correction|Character n-grams|

## 🔬 Word vs Character N-grams

|Type|Captures|Example for "hello"|
|---|---|---|
|Word N-grams|Phrase-level context|['hello world']|
|Char N-grams|Morphology, typos, subword info|['he', 'el', 'll', 'lo']|

```python
vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))
```

## ⚖️ Trade-offs of N-grams

|Pros|Cons|
|---|---|
|Captures local context|Can be **high-dimensional**|
|Useful for statistical language models|Doesn’t scale well with **large corpora**|
|Supports phrase modeling|Still ignores long-range dependencies|
|Simple to implement|Doesn’t capture semantics (like embeddings do)|

## Limitations

1. **Sparsity**: Higher-order N-grams lead to a very large, sparse matrix.
2. **Overfitting Risk**: Common in small datasets.
3. **No semantic similarity**: “good” ≠ “great”.
4. **Language-dependent tuning**: Optimal `n` varies by language and task.

## 🛠 Best Practices

- Start with **Unigrams + Bigrams**.
- Use **TF-IDF weighting** to avoid overemphasizing common phrases.
- For character-level analysis, try `ngram_range=(3, 5)` (trichar to 5-char).
- Limit vocabulary size with `max_features`.

## Summary

|Feature|Description|
|---|---|
|Captures|Word order (short-range)|
|Granularity|Word or character|
|Ideal for|Text classification, search, spell check|
|Downside|Large vocab, ignores semantics|

## What Comes After N-grams?

Once you've explored N-grams, it’s time to move into **vector-based representations** that capture **meaning**:

1. **Word Embeddings**
    - Word2Vec
    - GloVe
    - FastText
2. **Contextual Embeddings**
    - ELMo
    - BERT
