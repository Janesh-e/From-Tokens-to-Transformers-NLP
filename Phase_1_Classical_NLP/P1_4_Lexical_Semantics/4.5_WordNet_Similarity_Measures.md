# WordNet Similarity Measures

**WordNet Similarity Measures** helps machines **quantify the similarity between two word meanings (senses)** using the structure of the WordNet ontology.
These measures are fundamental in NLP tasks like **WSD, information retrieval, semantic search, QA systems**, etc.

These similarity metrics fall into **three categories**:

|Type|Description|Examples|
|---|---|---|
|**Path-based**|Use the structure of WordNet tree (taxonomy)|Path, Wu-Palmer, Leacock-Chodorow|
|**Information Content (IC)**|Use corpus statistics + taxonomy|Resnik, Jiang-Conrath, Lin|
|**Vector-based**|Use vectorized word embeddings|Cosine similarity|

---

## 1. **Path Similarity**

### Idea:

> Similarity = inverse of the **shortest path length** between two synsets in the WordNet hierarchy.

Formula:

```
similarity = 1 / (shortest_path_length + 1)
```

Range: [0, 1]

- 1 → same concept
- closer to 0 → unrelated

**Example**:  
If dog.n.01 and cat.n.01 have 3 steps between them,

$\text{similarity} = \frac{1}{3 + 1} = 0.25$

### Code Example:

```python
from nltk.corpus import wordnet as wn

dog = wn.synset('dog.n.01')          # domesticated canine
cat = wn.synset('cat.n.01')          # domesticated feline

similarity = dog.path_similarity(cat)
print(similarity)
```

Output:

```
0.2 (i.e., low similarity, but some shared ancestors)
```

**Limitation**: Doesn’t consider the depth of the synsets in the hierarchy.

---

## 2. **Wu-Palmer Similarity (wup_similarity)**

### Idea:

> Measures how **deep** the two synsets are and how **close** they are to their **Lowest Common Subsumer (LCS)** (most specific ancestor shared by both).

Formula:

```
similarity = (2 × depth(LCS)) / (depth(synset1) + depth(synset2))
```

Range: [0, 1]

- Higher = more similar.

**Example**:

- LCS = `mammal.n.01` with depth 5
- dog.n.01 at depth 7, cat.n.01 at depth 7

$\text{similarity} = \frac{2 \cdot 5}{7 + 7} = \frac{10}{14} \approx 0.714$

### Code Example:

```python
similarity = dog.wup_similarity(cat)
print(similarity)
```

Output:

```
0.8571 (relatively high – both are animals and have close ancestors)
```

**Better than Path Similarity** when it comes to **semantic closeness** and **depth-awareness**.

---

## 3. **Leacock-Chodorow Similarity (lch_similarity)**

### Idea:

> Based on shortest path between two synsets but **scaled by the taxonomy depth**.

Formula:

```
similarity = -log (shortest_path_length / (2 × D))
```

Where:
- `D` is the **maximum depth** of the hierarchy.

Range: [0 to ~3.6+] (higher = more similar)
**Smarter than path similarity: it accounts for WordNet's size.**

**Example**:

- Shortest path length = 4
- Max depth = 10

$\text{similarity} = -\log\left(\frac{4}{20}\right) = -\log(0.2) ≈ 1.609$

### Code Example:

```python
similarity = dog.lch_similarity(cat)
print(similarity)
```

Output:

```
2.0281 (scaled value based on path and depth)
```

Requires **both synsets to be nouns or both verbs**.

---

## 4. **Lin Similarity (lin_similarity)**

**Requires corpus-based IC (Information Content)**

### Idea:

> Measures similarity based on **shared information** in the taxonomy.

Formula:

```
sim = (2 × IC(LCS)) / (IC(synset1) + IC(synset2))
```

Where:
- **IC(x)** is the **information content** (how rare/precise the term is) derived from corpus probabilities
- LCS = Least Common Subsumer

Range: [0, 1]

- High = More shared meaning

**Interpretation**: More informative (less frequent) common ancestors → higher similarity.

### Code Example (with Brown corpus IC):

```python
from nltk.corpus import wordnet_ic
brown_ic = wordnet_ic.ic('ic-brown.dat')

similarity = dog.lin_similarity(cat, brown_ic)
print(similarity)
```

Output:

```
0.886 (meaningful overlap based on shared IC)
```

Very accurate for tasks involving **fine-grained sense similarity**

---

## 5. **Jiang-Conrath Distance (jcn_similarity)**

### Idea:

> Based on **information content distance** instead of similarity.

Formula:

```
distance = IC(synset1) + IC(synset2) - 2 × IC(LCS)
similarity = 1 / (distance + 1)
```

- Lower distance = higher similarity
- Converts to similarity by taking inverse

These require WordNet + a corpus (like Brown or SemCor) to calculate IC values.

### Code Example:

```python
similarity = dog.jcn_similarity(cat, brown_ic)
print(similarity)
```

Output:

```
0.449 or similar (depends on corpus IC values)
```

**Sensitive to IC granularity**, great for nuanced comparisons.

---

## 6. **Cosine Similarity** (Vector-based)

Measures the **angle between two word vectors** in a high-dimensional space.

> Used when words are embedded using **Word2Vec, GloVe, BERT**, etc.

Formula:

```
cos_sim = dot(A, B) / (||A|| * ||B||)
```

ie., $\text{similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \cdot \|B\|}$

Captures both **semantic and syntactic** similarity based on **usage patterns** in corpora.

### Code Example:

```python
import spacy
nlp = spacy.load("en_core_web_md")

doc1 = nlp("dog")
doc2 = nlp("cat")

print(doc1.similarity(doc2))
```

Output:

```
0.80+
```

- The value depends on vector model used.

Useful when you're outside WordNet and in the **neural embedding world**.

---

## Summary Table of WordNet Similarity Measures

| Method                 | Based on               | Range   | Requires IC? | Key Features     | Normalized? |
| ---------------------- | ---------------------- | ------- | ------------ | ---------------- | ----------- |
| **Path**               | Taxonomy path          | 0–1     | ❌            | Fast, simple     | ❌           |
| **Wu-Palmer**          | Depth + path           | 0–1     | ❌            | Depth-aware      | ✅           |
| **Leacock-Chodorow**   | Path + Max Depth       | ~0–3.6  | ❌            | Log-scaled       | ✅           |
| **Lin**                | Shared IC              | 0–1     | ✅            | Info overlap     | ✅           |
| **Jiang-Conrath**      | IC distance            | Inverse | ✅            | Fine-grained     | ❌           |
| **Cosine (Vec-based)** | Word embedding vectors | -1 to 1 | ❌            | Contextual usage | ✅           |

---

## When to Use What?

|Goal|Recommended Similarity|
|---|---|
|Quick structure-only measure|`wup_similarity`, `lch_similarity`|
|Corpus-aware semantic overlap|`lin_similarity`, `jcn_similarity`|
|Vector-based understanding|Cosine similarity (with embeddings)|
|Word Sense Disambiguation|Lesk + `wup`, `lin`, etc.|
