# **Recurrent Neural Networks (RNNs)** – Conceptual & Theoretical Understanding

## **What is an RNN?**

A **Recurrent Neural Network (RNN)** is a type of neural network designed specifically to handle **sequential data**, where the order of inputs matters. Unlike feedforward networks, where inputs and outputs are independent of each other, RNNs introduce a notion of _memory_ through **recurrent connections**.

The key idea is:  
The **output at any step depends not only on the current input but also on the previous computations.**

In simple terms:  
**RNN = Feedforward Network + Memory of the Past**

---

## **Why do we need RNNs in NLP?**

- Language is sequential: The meaning of words and sentences depends on context that comes from the sequence of words.
- Classical models (e.g., Bag of Words) ignore order and context — they treat a sentence as a bag of tokens.
- RNNs can learn **contextual relationships** — e.g., what adjectives describe what nouns, what subjects relate to what verbs.

Example:  
In the sentence:

> _"The concert was amazing and the crowd cheered loudly."_

An RNN can learn that _"cheered"_ relates to _"crowd"_, not _"concert"_, because of the sequence.

---

## **The RNN Architecture**

### The core mechanism:

At each time step `t`:

- The RNN takes input `x_t` (e.g., a word embedding at position `t`).
- It updates a hidden state `h_t`, which captures information from `x_t` and `h_{t-1}` (the hidden state from the previous step).
- It may produce an output `y_t`.

Mathematically:

$h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$ 
$y_t = g(W_{hy} h_t + b_y)$

Where:

- `x_t` → input at time step `t`
- `h_t` → hidden state at time step `t`
- `y_t` → output at time step `t`
- `W_{xh}, W_{hh}, W_{hy}` → weight matrices
- `b_h, b_y` → bias terms
- `f` → typically a nonlinearity like `tanh` or `ReLU`
- `g` → often `softmax` (for classification tasks)

The hidden state `h_t` acts as the **memory** of the network.

---

### **Unrolling the RNN**

Though the RNN looks like a small network, it is better visualized as being **unrolled over time**:

```
x_1 → [RNN cell]_1 → h_1 → y_1  
x_2 → [RNN cell]_2 → h_2 → y_2  
x_3 → [RNN cell]_3 → h_3 → y_3  
...
```

Each `[RNN cell]_t` shares the **same parameters** at every time step. This parameter sharing:  
✅ Reduces the number of parameters.  
✅ Allows generalization to sequences of different lengths.

---

## **How RNNs learn**

- The RNN is trained using **Backpropagation Through Time (BPTT)**.
- Gradients are computed for each time step, and the total gradient is the sum over all time steps.
- This allows the model to learn from entire sequences during training.

---

## **Challenges of RNNs**

While RNNs sound ideal in theory, they have significant limitations:

### ❗ **Vanishing gradients**

- When sequences are long, gradients during BPTT become very small.
- This prevents the network from learning dependencies that span many time steps.

### ❗ **Exploding gradients**

- Sometimes, gradients can become too large, destabilizing training.
- Gradient clipping is commonly used to address this.

### ❗ **Limited long-term memory**

- Standard RNNs struggle to capture dependencies that are far apart in the sequence (e.g., linking the subject of a long sentence to its verb).

---

## **Types of RNNs**

Depending on how inputs and outputs are structured:

|Type|Input|Output|Example use case|
|---|---|---|---|
|**Many-to-Many**|Sequence|Sequence|POS tagging, NER|
|**Many-to-One**|Sequence|Single output|Sentiment analysis|
|**One-to-Many**|Single input|Sequence|Image captioning|
|**Sequence-to-Sequence (Encoder-Decoder)**|Sequence|Sequence (diff length)|Machine translation|

---

## **Summary of RNN's strengths**

✅ Naturally models variable-length sequences.  
✅ Parameter sharing makes them efficient.  
✅ Captures local context effectively.  
✅ Foundation for more advanced models (LSTM, GRU, attention, Transformers).

---

## **Intuitive way to think about RNN**

Imagine you're reading a sentence word by word:

- After reading each word, you update your understanding of the sentence (hidden state).
- When you read the next word, your current understanding influences how you interpret it.

---
---

# **RNN Working Explained via Example**

---

## **Scenario**

Imagine we want to process the sentence:

> **"I love NLP"**

Suppose the task is _sentiment classification_ — we want the model to predict if the sentence is positive or negative.  
This is a **many-to-one RNN**: sequence in → single output (positive/negative).

---

## **Step 1: Representing the input**

Each word is first converted to a **vector** (e.g., via word embeddings). Let’s pretend for simplicity:

|Word|Vector (fake numbers for illustration)|
|---|---|
|I|[0.1, 0.3]|
|love|[0.7, 0.9]|
|NLP|[0.4, 0.5]|

Each word is now a 2-dimensional vector.  
So input = sequence of vectors:

$x_1 = [0.1, 0.3], \quad x_2 = [0.7, 0.9], \quad x_3 = [0.4, 0.5]$

---

## **Step 2: How the RNN processes the sequence**

Let’s go through each time step.

---

### **Time step 1 (word: "I")**

- Input: `x1 = [0.1, 0.3]`
- Initial hidden state: `h0 = [0, 0]` (usually zeros at start)

The RNN does:

$h_1 = \tanh(W_{xh} \cdot x_1 + W_{hh} \cdot h_0 + b_h)$

What this means:
- It multiplies `x1` by a weight matrix `W_xh`.
- It multiplies `h0` by a weight matrix `W_hh` (since `h0` is zero, this contributes nothing initially).
- Adds a bias.
- Passes through `tanh` (so output values stay between -1 and 1).

**Think of `h1` as a summary of what’s been seen so far** → just the word "I".

---

### **Time step 2 (word: "love")**

- Input: `x2 = [0.7, 0.9]`
- Hidden state from last step: `h1`

The RNN does:

$h_2 = \tanh(W_{xh} \cdot x_2 + W_{hh} \cdot h_1 + b_h)$

Now both:
- The new word's vector (`x2`)
- The memory of what we saw before (`h1`)  
    are combined to create `h2`.

**`h2` is a combined understanding of "I love"** — it encodes both past and present.

---

### **Time step 3 (word: "NLP")**

- Input: `x3 = [0.4, 0.5]`
- Hidden state from last step: `h2`

$h_3 = \tanh(W_{xh} \cdot x_3 + W_{hh} \cdot h_2 + b_h)$

Now `h3` represents the full sentence:

> **"I love NLP"**

It contains compressed information about all the words in order.

---

## **Final output**

For sentiment classification:

$\hat{y} = \text{softmax}(W_{hy} \cdot h_3 + b_y)$

The softmax converts the final hidden state into probabilities:

- e.g., 90% positive, 10% negative

---

## **Visualization of flow**

```
Time 1:
[I] → [x1=0.1,0.3] → [RNN cell] → h1 ("I")

Time 2:
[love] → [x2=0.7,0.9] + h1 → [RNN cell] → h2 ("I love")

Time 3:
[NLP] → [x3=0.4,0.5] + h2 → [RNN cell] → h3 ("I love NLP")

h3 → Output (positive/negative)
```

Each RNN cell keeps adding more information to the hidden state — like building a mental picture word-by-word.

---

## **What calculations happen at each step?**

Let’s break it intuitively:  
1️⃣ The current word vector is multiplied by a matrix → it decides **what this word means to the model**.  
2️⃣ The previous hidden state is multiplied by another matrix → it decides **how the new word changes our understanding of the sentence so far**.  
3️⃣ Both are added, plus bias, then squashed with `tanh` → it combines old and new info smoothly.

Hidden state = summary of everything seen so far.

---

## **Hidden state as memory**

Imagine reading:

- h1 = I have seen "I"
- h2 = I have seen "I love"
- h3 = I have seen "I love NLP"

Each hidden state builds on the last. The network tries to **compress the important parts of the sentence so far** into `h_t`.

---

## **Where it struggles**

Suppose instead we had a long sentence:

> "I have always thought that natural language processing is fascinating because it allows machines to understand human language and that is truly remarkable."

The distance between subject and verb (e.g., "processing" and "is") is large. By the time we get to the important verb, the early information may have faded — this is the **vanishing gradient problem**.

---

## **Summary of steps**

➡ Each word is converted to a vector.  
➡ The RNN cell updates its memory (hidden state) by combining new word info + old memory.  
➡ At the end, the last hidden state feeds into the final classifier (or further models).

---

## **Analogy**

Think of the RNN as a person listening to a sentence:

- Each word updates their understanding.
- They _remember_ important parts (via hidden state).
- At the end, they decide: "Was that positive or negative?"

---
---

# DETAILED DEMONSTRATION OF HOW THINGS WORK INTERNALLY : 

**Example Sentence from IMDb**

Let’s take this real example (from a positive review):

> "This movie was fantastic and I loved it."

---

#### 1️⃣ **Tokenization**

Tokenization splits the sentence into words or tokens.

➡ **Resulting tokens:**

```
["this", "movie", "was", "fantastic", "and", "i", "loved", "it", "."]
```

Each token is just a string.

---

#### 2️⃣ **Vocabulary mapping**

When we build the vocabulary (`vocab = build_vocab_from_iterator(...)`), each unique word in the dataset gets assigned an integer index.

➡ Suppose our vocab assigned indices like:

|word|index|
|---|---|
|`<unk>`|0|
|`this`|1|
|`movie`|2|
|`was`|3|
|`fantastic`|4|
|`and`|5|
|`i`|6|
|`loved`|7|
|`it`|8|
|`.`|9|
|…|…|

➡ So our sentence becomes:

```
[1, 2, 3, 4, 5, 6, 7, 8, 9]
```

This is just turning each word into a **symbolic number ID**.

_What does this mean?_  
➡ These numbers themselves don’t mean anything yet (e.g., “1” for `this` is just a placeholder).

---

#### 3️⃣ **Embedding lookup**

We can’t feed integers directly into the RNN. Instead, we map each word ID to a dense vector = **embedding**.

➡ Suppose our embedding size = 4 (to keep it small).  
Our embedding matrix is like:

|index|vector|
|---|---|
|1 (`this`)|[0.2, -0.1, 0.4, 0.6]|
|2 (`movie`)|[0.7, 0.5, -0.3, 0.1]|
|3 (`was`)|[0.0, 0.1, -0.1, 0.3]|
|4 (`fantastic`)|[0.8, 0.7, 0.6, 0.9]|
|5 (`and`)|[-0.2, 0.3, 0.1, 0.0]|
|6 (`i`)|[0.3, 0.2, -0.2, 0.4]|
|7 (`loved`)|[0.9, 0.8, 0.7, 1.0]|
|8 (`it`)|[0.1, 0.0, 0.2, 0.3]|
|9 (`.`)|[0.05, -0.05, 0.0, 0.1]|

➡ So now our sentence becomes:

```
[
 [0.2, -0.1, 0.4, 0.6],  # this
 [0.7, 0.5, -0.3, 0.1],  # movie
 [0.0, 0.1, -0.1, 0.3],  # was
 [0.8, 0.7, 0.6, 0.9],  # fantastic
 [-0.2, 0.3, 0.1, 0.0], # and
 [0.3, 0.2, -0.2, 0.4], # i
 [0.9, 0.8, 0.7, 1.0],  # loved
 [0.1, 0.0, 0.2, 0.3],  # it
 [0.05, -0.05, 0.0, 0.1] # .
]
```

Each row = a vector representing a word’s meaning (learned during training).

_What do these vectors mean?_  
➡ Initially, they’re random — during training they adjust so:

- Similar words have similar vectors (e.g. `fantastic` close to `great`)
- Dissimilar words far apart

---

#### 4️⃣ **Final input to RNN**

Now our input is:

```
A matrix of shape: (sequence_length=9, embedding_size=4)
```

Example:

```
[
 [0.2, -0.1, 0.4, 0.6],
 [0.7, 0.5, -0.3, 0.1],
 ...
]
```

The RNN will process this _one row at a time_, updating its hidden state at each step.

---

#### **Summary diagram**

```
Sentence:  "This movie was fantastic and I loved it."

⬇ Tokenization
["this", "movie", "was", "fantastic", "and", "i", "loved", "it", "."]

⬇ Vocab mapping
[1, 2, 3, 4, 5, 6, 7, 8, 9]

⬇ Embedding lookup
[
 [0.2, -0.1, 0.4, 0.6],
 [0.7, 0.5, -0.3, 0.1],
 ...
]
```

**These embeddings are the true input to the RNN!**

---

#### **What sense do the numbers make?**

- The numbers in embeddings are _latent features_ the model uses to compute patterns.
- A word like `fantastic` might have a large positive value on the feature that encodes “positivity”.
- Words like `and`, `the`, `was` might have small values — they don’t contribute much sentiment.

---

### Now let's see what RNN does with these data!

> **"This movie was fantastic and I loved it."**

Let’s focus on the **first word: `"this"`**

---

#### 1️⃣ **Embedding vector for "this"**

Suppose embedding size = 4.

Embedding vector (chosen for easy math):

$x_1 = \begin{bmatrix} 0.2 \\ -0.1 \\ 0.4 \\ 0.6 \end{bmatrix}$

This represents our word `"this"` as 4 features (learned during training).

---

#### 2️⃣ **Initial hidden state**

Since it’s the first word:

$h_0 = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$

(hidden size = 3 in this example)

Hidden state starts at zero — it’ll accumulate info as we read the sequence.

---

#### 3️⃣ **Weight matrices**

Randomly initialized, trained during learning.

We have:

$W: \text{ (hidden size 3) x (embed size 4)}$
$U: \text{ (hidden size 3) x (hidden size 3)}$
$b: \text{ (hidden size 3)}$

Let’s pick values:

$W = \begin{bmatrix} 0.1 & 0.2 & 0.0 & -0.1 \\ -0.3 & 0.0 & 0.2 & 0.1 \\ 0.05 & -0.2 & 0.1 & 0.2 \end{bmatrix}$ 
$U = \begin{bmatrix} 0.1 & 0.0 & 0.2 \\ 0.0 & 0.3 & -0.1 \\ -0.2 & 0.1 & 0.0 \end{bmatrix}$
$b = \begin{bmatrix} 0.01 \\ -0.02 \\ 0.03 \end{bmatrix}$

---

#### 4️⃣ **Compute W @ x**

```
W @ x_1 = 
[
  0.1*0.2 + 0.2*(-0.1) + 0.0*0.4 + (-0.1)*0.6,
  -0.3*0.2 + 0.0*(-0.1) + 0.2*0.4 + 0.1*0.6,
  0.05*0.2 + (-0.2)*(-0.1) + 0.1*0.4 + 0.2*0.6
]
```

Let’s compute:

- Row 1:
    ```
    = 0.02 - 0.02 + 0.0 - 0.06 = -0.06
    ```
    
- Row 2:
    ```
    = -0.06 + 0.0 + 0.08 + 0.06 = 0.08
    ```
    
- Row 3:
    ```
    = 0.01 + 0.02 + 0.04 + 0.12 = 0.19
    ```


$W @ x_1 = \begin{bmatrix} -0.06 \\ 0.08 \\ 0.19 \end{bmatrix}$

---

#### 5️⃣ **Compute U @ h**

Since h₀ = 0:

$U @ h_0 = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$

---

#### 6️⃣ **Sum all parts**

$W @ x_1 + U @ h_0 + b = \begin{bmatrix} -0.06 \\ 0.08 \\ 0.19 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + \begin{bmatrix} 0.01 \\ -0.02 \\ 0.03 \end{bmatrix} = \begin{bmatrix} -0.06 + 0.01 = -0.05 \\ 0.08 - 0.02 = 0.06 \\ 0.19 + 0.03 = 0.22 \end{bmatrix}$

---

#### 7️⃣ **Apply activation function (tanh)**

tanh squashes values between -1 and 1  
tanh(x) ≈ x if x small, otherwise non-linear

Let’s compute tanh element-wise:

```
tanh(-0.05) ≈ -0.05
tanh(0.06) ≈ 0.06
tanh(0.22) ≈ 0.22
```

$h_1 = \begin{bmatrix} -0.05 \\ 0.06 \\ 0.22 \end{bmatrix}$

---

#### **What happened?**

The RNN combined:

- info from `"this"` (embedding vector)
- previous hidden state (was 0, so no effect now)
- biases

Output: a hidden state `h1` that carries forward _context_ at this step.

---

#### **What do these values mean?**

```
h1 = [-0.05, 0.06, 0.22]
```

- These are the internal memory of the network after seeing `"this"`.
- Think of each hidden unit as a learned feature detector:
    - One might accumulate info about sentiment
    - Another might track subject/object info
    - Another could detect position/patterns

---

### Let's proceed with the next word 

- Next word: `"movie"`

#### 1️⃣ **Embedding vector for `"movie"`**

➡ From our earlier table:

$x_2 = \begin{bmatrix} 0.7 \\ 0.5 \\ -0.3 \\ 0.1 \end{bmatrix}$

---

#### 2️⃣ **Compute W @ x₂**

```
W @ x₂ = 
[
0.1*0.7 + 0.2*0.5 + 0.0*(-0.3) + (-0.1)*0.1,
-0.3*0.7 + 0.0*0.5 + 0.2*(-0.3) + 0.1*0.1,
0.05*0.7 + (-0.2)*0.5 + 0.1*(-0.3) + 0.2*0.1
]
```

Let’s compute:

- Row 1:
    ```
    = 0.07 + 0.10 + 0.0 - 0.01 = 0.16
    ```
    
- Row 2:
    ```
    = -0.21 + 0.0 - 0.06 + 0.01 = -0.26
    ```
    
- Row 3:
    ```
    = 0.035 - 0.10 - 0.03 + 0.02 = -0.075
    ```

$W @ x_2 = \begin{bmatrix} 0.16 \\ -0.26 \\ -0.075 \end{bmatrix}$

---

#### 3️⃣ **Compute U @ h₁**

```
U @ h₁ = 
[
0.1*(-0.05) + 0.0*0.06 + 0.2*0.22,
0.0*(-0.05) + 0.3*0.06 + (-0.1)*0.22,
-0.2*(-0.05) + 0.1*0.06 + 0.0*0.22
]
```

Let’s compute:

- Row 1:
    ```
    = -0.005 + 0.0 + 0.044 = 0.039
    ```
    
- Row 2:
    ```
    = 0.0 + 0.018 - 0.022 = -0.004
    ```
    
- Row 3:
    ```
    = 0.01 + 0.006 + 0.0 = 0.016
    ```

$U @ h_1 = \begin{bmatrix} 0.039 \\ -0.004 \\ 0.016 \end{bmatrix}$

---

#### 4️⃣ **Sum W @ x₂ + U @ h₁ + b**

```
= 
[
0.16 + 0.039 + 0.01,
-0.26 - 0.004 - 0.02,
-0.075 + 0.016 + 0.03
]
```

$\begin{bmatrix} 0.209 \\ -0.284 \\ -0.029 \end{bmatrix}$

---

#### 5️⃣ **Apply tanh**

Let’s compute:

```
tanh(0.209) ≈ 0.206
tanh(-0.284) ≈ -0.277
tanh(-0.029) ≈ -0.029
```

Final hidden state:

$h_2 = \begin{bmatrix} 0.206 \\ -0.277 \\ -0.029 \end{bmatrix}$

---

#### **What just happened?**

The RNN combined:

- The embedding info for `"movie"`
- The memory from `"this"`  
     Result: a new hidden state `h₂`  
     `h₂` holds both:
- What we’ve seen so far: `"this movie"`
- Internal features like sentiment, structure etc.

---

### **Why do we use the same `W` and `U` for all time steps?**

- This is the essence of **weight sharing** in RNNs.
- The RNN is designed to process sequences of _any_ length with the _same_ set of parameters.
- Using the same `W` and `U` lets the model generalize the same transformation logic at each position — whether it's the first word or the 50th word.

### **What changes at each time step then?**

➡ The _inputs_ (`xₜ`) change — because each word is different.  
➡ The _hidden state_ (`hₜ₋₁`) changes — because memory evolves as more words are seen.

But the matrices `W`, `U`, and `b` are the same, applying the same rule at each step:

$h_t = \tanh(W x_t + U h_{t-1} + b)$

### **When do `W` and `U` change?**

They are updated during **training**, not during sequence processing (inference or forward pass).

➡ After you process a full sequence (or a batch of sequences):

- Compute loss (e.g., cross-entropy for sentiment)
- Use gradient descent (via backpropagation through time, BPTT) to adjust `W`, `U`, and `b`
- This means the RNN _learns_ better weights over many sequences

#### **Analogy**

Imagine `W` and `U` as _universal tools_ the RNN uses at each step — like the same wrench and screwdriver being used on each part of a machine, no matter the part’s position.  
What the tool does at each part depends on the part’s shape (the input and hidden state), not on switching tools.

---
---

# **Big Picture Flow of RNN Processing**

#### 2️⃣ **Hidden state flow**

We process word by word:

```
h0 = [0, 0, 0]  # starting hidden state
```

For each timestep `t`:

$h_t = \tanh(W x_t + U h_{t-1} + b)$

➡ What’s happening:

- `W x_t`: extracts features from the word embedding (e.g. "is this a positive word?")
- `U h_{t-1}`: passes along memory from previous context
- `b`: adjusts with learned bias
- `tanh`: keeps values in range (-1, 1), introduces nonlinearity

This builds up a summary of the sequence up to that word.

---

#### 3️⃣ **Example hidden state evolution**

|Word|Hidden state (example values)|
|---|---|
|this|[-0.05, 0.06, 0.22]|
|movie|[0.206, -0.277, -0.029]|
|was|[0.15, -0.3, 0.1]|
|fantastic|[0.6, -0.1, 0.3]|
|and|[0.5, -0.2, 0.25]|
|I|[0.4, -0.25, 0.2]|
|loved|[0.7, -0.05, 0.4]|
|it|[0.65, -0.1, 0.35]|

Notice how hidden states change — positive words like `"fantastic"` or `"loved"` might push certain dimensions up!

---

#### 4️⃣ **Final hidden state**

After the last word, `h_T` is like a _summary_ of the whole sentence’s meaning.  
In a sentiment task:

```
h_T → fully connected layer → sigmoid or softmax → positive/negative score
```

---

#### 5️⃣ **What are the RNN’s hidden units learning?**

Each dimension in the hidden state might encode:

- Sentiment polarity
- Subject-verb consistency
- Sequence order info (e.g. is this the end of a clause?)
- Emphasis / intensity of words

Over training, the model learns how to use these hidden units to make good predictions.

---

#### **Diagram of sequence processing**

```
(this)  -->  (movie)  -->  (was)  -->  (fantastic) ...
  |          |           |             |
 [h1] --> [h2] --> [h3] --> [h4] --> ...
```

Each arrow means:

- Compute `W x_t + U h_{t-1} + b`
- Apply `tanh`
- Update hidden state

Final `h_T` feeds into classifier.

---

#### **In short**

At each step:

- The RNN updates its memory (hidden state) based on:
    - current word’s features (embedding)
    - what it has seen so far (previous hidden state)

By the end:

- The final hidden state is a compressed summary of the whole sentence — ready for classification, generation, etc.

---

## **RNN hidden state flow (conceptual diagram)**

```
 ┌───────────┐      ┌───────────┐      ┌───────────┐      ┌───────────┐
 │Embedding: │      │Embedding: │      │Embedding: │      │Embedding: │
 │"This"     │      │"movie"    │      │"was"      │      │"fantastic"│
 └────┬──────┘      └────┬──────┘      └────┬──────┘      └────┬──────┘
      │                 │                 │                 │
      ▼                 ▼                 ▼                 ▼
 ┌──────────┐        ┌──────────┐        ┌──────────┐        ┌──────────┐
 │ h₁       │───────▶│ h₂       │───────▶│ h₃       │───────▶│ h₄       │
 │ summary  │        │ summary  │        │ summary  │        │ summary  │
 │ of "This"│        │ of "This │        │ of "This │        │ of "This │
 │          │        │ movie"   │        │ movie was│        │ movie was│
 │          │        │          │        │          │        │ fantastic│
 └──────────┘        └──────────┘        └──────────┘        └──────────┘
```

```
       │                      │
       ▼                      ▼
     ...                    ...
```

```
 ┌──────────┐        ┌──────────┐        ┌──────────┐        ┌──────────┐
 │ h₅       │───────▶│ h₆       │───────▶│ h₇       │───────▶│ h₈       │
 │ "and"    │        │ "I"      │        │ "loved"  │        │ "it"     │
 └──────────┘        └──────────┘        └──────────┘        └──────────┘
```

---

## **What happens at each node (`h₁`, `h₂`, ..., `h₈`)?**

Each hidden state stores:

- What the RNN has understood about the sentence _so far_
- A mixture of word meaning + order + memory of important features

Mathematically:

$h_t = \tanh(W x_t + U h_{t-1} + b)$

The arrows:

- Flow of memory / sequence understanding through time

---

## **Final step**

```
 h₈ ───▶ Fully Connected ───▶ Sentiment (positive/negative)
```

The final hidden state `h₈` contains the sentence’s meaning  
Passed into a classifier head

---

#### **Key points this diagram shows**

- Inputs are fed one at a time → RNN keeps memory through `h_t`
- Arrows = flow of memory, evolving understanding
- Final state `h8` = full sentence encoding

---
---

# **What are gradients in an RNN?**

When we train an RNN:

- We compute the loss at the end of the sequence (e.g. after predicting sentiment).
- We backpropagate this error _back through time_ (through each hidden state).
- At each time step, we compute gradients of the loss w.r.t. parameters (`W`, `U`, `b`) and hidden states.

The gradient at each time step is affected by:

$\frac{\partial L}{\partial h_t} = \frac{\partial L}{\partial h_T} \prod_{k=t+1}^T \frac{\partial h_k}{\partial h_{k-1}}$

That product of derivatives causes the issue!

---

### **Let’s work through an example**

Let’s imagine:

- Sequence length = 5
- Each `∂h_k / ∂h_{k-1}` (the derivative matrix) has values near some scalar factor (let’s simplify and use scalar intuition)

---

### 🔹 **Case 1: Vanishing gradients**

Suppose each derivative looks like:

$\frac{\partial h_k}{\partial h_{k-1}} \approx 0.5$

Gradient at time step 1:

$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_5} \times 0.5 \times 0.5 \times 0.5 \times 0.5 = \frac{\partial L}{\partial h_5} \times 0.5^4 = \frac{\partial L}{\partial h_5} \times 0.0625$

By the time we reach the start of the sequence, the gradient has shrunk a lot (0.0625× the original).

If the sequence were longer (say 20 steps):

$0.5^{20} = 9.5 \times 10^{-7}$

⚠ The gradient is almost 0 — this means:

- Early layers learn _very slowly_ (or not at all)
- The model struggles to capture long-range dependencies (like the first word influencing the last)

---

### 🔹 **Case 2: Exploding gradients**

Suppose each derivative looks like:

$\frac{\partial h_k}{\partial h_{k-1}} \approx 1.5$

Gradient at time step 1:

$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_5} \times 1.5^4 = \frac{\partial L}{\partial h_5} \times 5.06$

If sequence = 20:

$1.5^{20} \approx 3.3 \times 10^3$

The gradient _explodes_ — becomes huge.

⚠ The problem:

- Causes numerical instability
- Weights can overshoot during update → training fails (NaN loss or wildly oscillating values)

---

### **Why is this a problem?**

|Issue|Effect|
|---|---|
|Vanishing gradient|Early time steps don’t learn well → model forgets long-term info|
|Exploding gradient|Loss jumps around, hard to converge, risk of numerical overflow|

---

### **What’s the root cause?**

The key reason:

$\prod_{k} \frac{\partial h_k}{\partial h_{k-1}}$

is a product of many terms (matrix multiplications)  
➡ If the largest singular value of `∂h_k/∂h_{k-1}` is:

- < 1 → gradient decays exponentially
- > 1 → gradient grows exponentially

---

### **Summary**

✅ **Vanishing gradient:** Each derivative < 1 → gradient gets tiny → can’t learn long-term patterns  
✅ **Exploding gradient:** Each derivative > 1 → gradient gets huge → training unstable

---
---

