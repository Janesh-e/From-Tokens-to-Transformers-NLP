# **Introduction to Sequence Models**

In natural language, meaning is not derived from words in isolation — it emerges from the **order** and **relationships** between words in a sequence. Consider the difference between:

- _“The cat chased the mouse.”_
- _“The mouse chased the cat.”_

Though both sentences contain the same words, the meaning changes entirely because of the sequence of those words.

Traditional models like **Bag of Words (BoW)** or **TF-IDF** discard this sequential information. This is where **Sequence Models** come in — these models are designed to process data where **order matters**, such as:

- Sentences, paragraphs, documents (NLP)
- Audio signals (speech processing)
- Time series (stock prices, sensor data)

In NLP, **Sequence Models** give us the tools to model the dependencies between words, learn context, and generate meaningful outputs that respect word order.

---

## **Goals of this submodule**

In this submodule, we aim to:  
✅ Understand how neural networks can process sequences of variable length.  
✅ Learn how information flows through sequences and how models "remember" past information.  
✅ Study different sequence architectures — their strengths, limitations, and use cases.  
✅ Implement these models in code (using PyTorch or TensorFlow/Keras).

---

## **What we'll cover in detail**

|Topic|What you'll learn|
|---|---|
|**Recurrent Neural Networks (RNN)**|The basic building block of sequence models. How RNNs pass hidden state information through a sequence. Why they struggle with long-term dependencies.|
|**Long Short-Term Memory (LSTM)**|How LSTMs solve the vanishing gradient problem and learn longer dependencies. The role of gates (input, forget, output).|
|**Gated Recurrent Unit (GRU)**|A simpler alternative to LSTM with comparable performance in many tasks. How GRUs reduce complexity while retaining key functionality.|
|**Variants & Best Practices**|Techniques like bidirectional RNNs/LSTMs, stacked layers, dropout in sequence models, sequence padding, masking.|

---

## **Why do we need specialized sequence models?**

Unlike standard feedforward networks, where input and output are fixed-size vectors, sequence models can:

- Accept inputs of varying length (e.g., sentences of different word counts).
- Maintain a notion of _time_ or _position_ through hidden states.
- Produce outputs that are sequences (e.g., text generation, translation) or single predictions (e.g., sentiment classification).

Without these models, tasks like translation, speech recognition, or chatbot dialogue would not be feasible at high quality.

---

## **Key challenges we'll explore**

- How do models "remember" important information over long sequences?
- How do we train these models efficiently despite issues like **vanishing gradients**?
- How can we balance performance and computational cost?

---

## **What's next?**

We will start our journey with the **Recurrent Neural Network (RNN)** — the simplest form of sequence model — to understand the core principles. From there, we will explore its limitations and how LSTM and GRU address them.

---

_Tip for learners: Whenever possible, visualize the hidden states and outputs of your models during training — it helps in building intuition about how sequences are processed._
