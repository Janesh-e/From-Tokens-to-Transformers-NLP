# **Recurrent Neural Networks (RNNs)** – Conceptual & Theoretical Understanding

## **What is an RNN?**

A **Recurrent Neural Network (RNN)** is a type of neural network designed specifically to handle **sequential data**, where the order of inputs matters. Unlike feedforward networks, where inputs and outputs are independent of each other, RNNs introduce a notion of _memory_ through **recurrent connections**.

The key idea is:  
The **output at any step depends not only on the current input but also on the previous computations.**

In simple terms:  
**RNN = Feedforward Network + Memory of the Past**

---

## **Why do we need RNNs in NLP?**

- Language is sequential: The meaning of words and sentences depends on context that comes from the sequence of words.
- Classical models (e.g., Bag of Words) ignore order and context — they treat a sentence as a bag of tokens.
- RNNs can learn **contextual relationships** — e.g., what adjectives describe what nouns, what subjects relate to what verbs.

Example:  
In the sentence:

> _"The concert was amazing and the crowd cheered loudly."_

An RNN can learn that _"cheered"_ relates to _"crowd"_, not _"concert"_, because of the sequence.

---

## **The RNN Architecture**

### The core mechanism:

At each time step `t`:

- The RNN takes input `x_t` (e.g., a word embedding at position `t`).
- It updates a hidden state `h_t`, which captures information from `x_t` and `h_{t-1}` (the hidden state from the previous step).
- It may produce an output `y_t`.

Mathematically:

$h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$ 
$y_t = g(W_{hy} h_t + b_y)$

Where:

- `x_t` → input at time step `t`
- `h_t` → hidden state at time step `t`
- `y_t` → output at time step `t`
- `W_{xh}, W_{hh}, W_{hy}` → weight matrices
- `b_h, b_y` → bias terms
- `f` → typically a nonlinearity like `tanh` or `ReLU`
- `g` → often `softmax` (for classification tasks)

The hidden state `h_t` acts as the **memory** of the network.

---

### **Unrolling the RNN**

Though the RNN looks like a small network, it is better visualized as being **unrolled over time**:

```
x_1 → [RNN cell]_1 → h_1 → y_1  
x_2 → [RNN cell]_2 → h_2 → y_2  
x_3 → [RNN cell]_3 → h_3 → y_3  
...
```

Each `[RNN cell]_t` shares the **same parameters** at every time step. This parameter sharing:  
✅ Reduces the number of parameters.  
✅ Allows generalization to sequences of different lengths.

---

## **How RNNs learn**

- The RNN is trained using **Backpropagation Through Time (BPTT)**.
- Gradients are computed for each time step, and the total gradient is the sum over all time steps.
- This allows the model to learn from entire sequences during training.

---

## **Challenges of RNNs**

While RNNs sound ideal in theory, they have significant limitations:

### ❗ **Vanishing gradients**

- When sequences are long, gradients during BPTT become very small.
- This prevents the network from learning dependencies that span many time steps.

### ❗ **Exploding gradients**

- Sometimes, gradients can become too large, destabilizing training.
- Gradient clipping is commonly used to address this.

### ❗ **Limited long-term memory**

- Standard RNNs struggle to capture dependencies that are far apart in the sequence (e.g., linking the subject of a long sentence to its verb).

---

## **Types of RNNs**

Depending on how inputs and outputs are structured:

|Type|Input|Output|Example use case|
|---|---|---|---|
|**Many-to-Many**|Sequence|Sequence|POS tagging, NER|
|**Many-to-One**|Sequence|Single output|Sentiment analysis|
|**One-to-Many**|Single input|Sequence|Image captioning|
|**Sequence-to-Sequence (Encoder-Decoder)**|Sequence|Sequence (diff length)|Machine translation|

---

## **Summary of RNN's strengths**

✅ Naturally models variable-length sequences.  
✅ Parameter sharing makes them efficient.  
✅ Captures local context effectively.  
✅ Foundation for more advanced models (LSTM, GRU, attention, Transformers).

---

## **Intuitive way to think about RNN**

Imagine you're reading a sentence word by word:

- After reading each word, you update your understanding of the sentence (hidden state).
- When you read the next word, your current understanding influences how you interpret it.

---
---

# **RNN Working Explained via Example**

---

## **Scenario**

Imagine we want to process the sentence:

> **"I love NLP"**

Suppose the task is _sentiment classification_ — we want the model to predict if the sentence is positive or negative.  
This is a **many-to-one RNN**: sequence in → single output (positive/negative).

---

## **Step 1: Representing the input**

Each word is first converted to a **vector** (e.g., via word embeddings). Let’s pretend for simplicity:

|Word|Vector (fake numbers for illustration)|
|---|---|
|I|[0.1, 0.3]|
|love|[0.7, 0.9]|
|NLP|[0.4, 0.5]|

Each word is now a 2-dimensional vector.  
So input = sequence of vectors:

$x_1 = [0.1, 0.3], \quad x_2 = [0.7, 0.9], \quad x_3 = [0.4, 0.5]$

---

## **Step 2: How the RNN processes the sequence**

Let’s go through each time step.

---

### **Time step 1 (word: "I")**

- Input: `x1 = [0.1, 0.3]`
- Initial hidden state: `h0 = [0, 0]` (usually zeros at start)

The RNN does:

$h_1 = \tanh(W_{xh} \cdot x_1 + W_{hh} \cdot h_0 + b_h)$

What this means:
- It multiplies `x1` by a weight matrix `W_xh`.
- It multiplies `h0` by a weight matrix `W_hh` (since `h0` is zero, this contributes nothing initially).
- Adds a bias.
- Passes through `tanh` (so output values stay between -1 and 1).

**Think of `h1` as a summary of what’s been seen so far** → just the word "I".

---

### **Time step 2 (word: "love")**

- Input: `x2 = [0.7, 0.9]`
- Hidden state from last step: `h1`

The RNN does:

$h_2 = \tanh(W_{xh} \cdot x_2 + W_{hh} \cdot h_1 + b_h)$

Now both:
- The new word's vector (`x2`)
- The memory of what we saw before (`h1`)  
    are combined to create `h2`.

**`h2` is a combined understanding of "I love"** — it encodes both past and present.

---

### **Time step 3 (word: "NLP")**

- Input: `x3 = [0.4, 0.5]`
- Hidden state from last step: `h2`

$h_3 = \tanh(W_{xh} \cdot x_3 + W_{hh} \cdot h_2 + b_h)$

Now `h3` represents the full sentence:

> **"I love NLP"**

It contains compressed information about all the words in order.

---

## **Final output**

For sentiment classification:

$\hat{y} = \text{softmax}(W_{hy} \cdot h_3 + b_y)$

The softmax converts the final hidden state into probabilities:

- e.g., 90% positive, 10% negative

---

## **Visualization of flow**

```
Time 1:
[I] → [x1=0.1,0.3] → [RNN cell] → h1 ("I")

Time 2:
[love] → [x2=0.7,0.9] + h1 → [RNN cell] → h2 ("I love")

Time 3:
[NLP] → [x3=0.4,0.5] + h2 → [RNN cell] → h3 ("I love NLP")

h3 → Output (positive/negative)
```

Each RNN cell keeps adding more information to the hidden state — like building a mental picture word-by-word.

---

## **What calculations happen at each step?**

Let’s break it intuitively:  
1️⃣ The current word vector is multiplied by a matrix → it decides **what this word means to the model**.  
2️⃣ The previous hidden state is multiplied by another matrix → it decides **how the new word changes our understanding of the sentence so far**.  
3️⃣ Both are added, plus bias, then squashed with `tanh` → it combines old and new info smoothly.

Hidden state = summary of everything seen so far.

---

## **Hidden state as memory**

Imagine reading:

- h1 = I have seen "I"
- h2 = I have seen "I love"
- h3 = I have seen "I love NLP"

Each hidden state builds on the last. The network tries to **compress the important parts of the sentence so far** into `h_t`.

---

## **Where it struggles**

Suppose instead we had a long sentence:

> "I have always thought that natural language processing is fascinating because it allows machines to understand human language and that is truly remarkable."

The distance between subject and verb (e.g., "processing" and "is") is large. By the time we get to the important verb, the early information may have faded — this is the **vanishing gradient problem**.

---

## **Summary of steps**

➡ Each word is converted to a vector.  
➡ The RNN cell updates its memory (hidden state) by combining new word info + old memory.  
➡ At the end, the last hidden state feeds into the final classifier (or further models).

---

## **Analogy**

Think of the RNN as a person listening to a sentence:

- Each word updates their understanding.
- They _remember_ important parts (via hidden state).
- At the end, they decide: "Was that positive or negative?"

---
---

