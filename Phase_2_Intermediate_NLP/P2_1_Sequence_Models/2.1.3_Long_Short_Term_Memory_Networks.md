# **Long Short-Term Memory (LSTM) Networks: Introduction**

In the world of sequence modeling, **Recurrent Neural Networks (RNNs)** laid the foundation for processing data with temporal dependencies — such as sentences, speech, or time-series signals. RNNs are designed to handle inputs of variable lengths and maintain a form of "memory" through their hidden states, enabling them to connect information across time steps.

However, despite their elegant design, **standard RNNs struggle to capture long-range dependencies in practice**. Let’s explore why, and how **Long Short-Term Memory (LSTM)** networks were introduced as a solution.

---

## **Limitations of Standard RNNs**

1️⃣ **Vanishing Gradient Problem**

- When we train RNNs using backpropagation through time (BPTT), gradients need to flow backward through many time steps.
- If the gradients are repeatedly multiplied by small numbers (due to derivatives of activation functions or weight matrices), they shrink exponentially.
- ➡ Result: Early time steps receive extremely small gradients, preventing the model from learning long-term dependencies.

2️⃣ **Exploding Gradient Problem**

- Conversely, if gradients are repeatedly multiplied by large numbers, they grow exponentially.
- ➡ Result: Training becomes numerically unstable, causing wildly fluctuating loss values and failed convergence.

3️⃣ **Short Memory**

- Even beyond gradient issues, the simple recurrence formula of RNNs struggles to effectively remember information over long sequences.
- The hidden state `h_t` gets overwritten as new inputs arrive, making it hard to preserve important context from the distant past.

---

## **The LSTM Solution**

**Long Short-Term Memory (LSTM)** networks were introduced by _Hochreiter and Schmidhuber (1997)_ to directly address these shortcomings.

**What makes LSTM different?**

- LSTMs introduce a more sophisticated "memory cell" structure with built-in mechanisms to control the flow of information.
- Instead of simply updating the hidden state with each new input, LSTMs _decide_ what to keep, what to forget, and what to output.
- These decisions are made using special structures called **gates**.

---

## **Key Components of LSTM**

|Component|Purpose|
|---|---|
|**Cell state (`C_t`)**|The main "memory" of the network that carries information across time steps with minimal modification. Designed to reduce vanishing gradient risk.|
|**Forget gate**|Decides what part of the cell state to erase (forget irrelevant or outdated info).|
|**Input gate**|Determines which new information should be added to the cell state.|
|**Output gate**|Controls what part of the memory will influence the output at the current time step.|

By combining these gates, the LSTM carefully regulates the flow of information, maintaining long-term memory when necessary and forgetting when appropriate.

---

## **How LSTM solves RNN problems**

|Problem in RNN|How LSTM addresses it|
|---|---|
|Vanishing gradients|Cell state enables gradients to flow unchanged (or minimally changed) across many time steps. Gates help prevent uncontrolled shrinking of gradients.|
|Exploding gradients|Controlled updates via gates prevent gradients from growing exponentially.|
|Poor long-term memory|The cell state can preserve information across long sequences, with gates managing when to update or reset memory.|

---

## **Why LSTMs became the go-to for sequence learning**

✅ They capture **both short-term and long-term dependencies** effectively.  
✅ They enable successful training on long sequences (e.g., long sentences, paragraphs, speech, video).  
✅ They are the foundation for many state-of-the-art models in NLP, speech recognition, and beyond.

---

### **In summary**

> **LSTMs enhance the basic RNN by introducing memory cells and gates that actively control information flow. This architecture overcomes key limitations of standard RNNs, making it possible to model sequences with both short- and long-range dependencies effectively.**

---
---

# **LSTM: Conceptual & Theoretical Understanding**

---

## **The Big Picture**

LSTM (Long Short-Term Memory) networks were created to _solve the memory problem_ in standard RNNs:

- Standard RNNs mix up old and new information at each step → hard to remember distant past.
- LSTMs introduce a **cell state** (the memory) and **gates** (controllers) that regulate information flow:
    -  What to forget
    -  What to add
    -  What to output

Think of LSTM as:
> **A conveyor belt (cell state) running through the network, with gates controlling what is placed on, removed from, or read off the belt at each time step.**

---

## **Core LSTM Components**

Let’s break down what happens at _each time step_:

### 🔹 **Cell State (`C_t`)**

- A highway for memory.
- Runs through all time steps with minor modifications.
- Allows gradients to flow without vanishing or exploding.

---

### 🔹 **Hidden State (`h_t`)**

- The output of the LSTM at the current time step.
- Like RNN’s `h_t`, but computed in a smarter way using the gates.

---

### 🔹 **Gates**

LSTMs have _three_ gates — think of them as "valves" with values between 0 (closed) and 1 (fully open):

|Gate|Purpose|Formula shape|
|---|---|---|
|**Forget gate (`f_t`)**|How much of the previous cell state should we keep?|`f_t = σ(W_f · [h_{t-1}, x_t] + b_f)`|
|**Input gate (`i_t`)**|How much new info should we add to the cell state?|`i_t = σ(W_i · [h_{t-1}, x_t] + b_i)`|
|**Candidate (`ĝ_t`)**|What new candidate values could go into the cell state?|`ĝ_t = tanh(W_g · [h_{t-1}, x_t] + b_g)`|
|**Output gate (`o_t`)**|How much of the cell state should we output?|`o_t = σ(W_o · [h_{t-1}, x_t] + b_o)`|

---

## **Step-by-Step Flow**

Let’s follow a word at position `t`:

At time step `t`, the LSTM cell receives:

- `x_t`: current input (e.g., word embedding at time `t`)
- `h_{t-1}`: hidden state from previous time step
- `C_{t-1}`: cell state from previous time step

We combine `x_t` and `h_{t-1}` (often concatenated or matrix multiplied) for calculations.

---

### 1️⃣ Forget gate:

$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

- `W_f`: weight matrix for forget gate
- `b_f`: bias for forget gate
- `σ`: sigmoid activation → squashes values between 0 and 1

- This produces values (0-1) that decide how much of each piece of `C_{t-1}` to keep.
- `f_t ≈ 1` → keep info
- `f_t ≈ 0` → forget info
- Example: If `f_t = [1, 0.7, 0.2]`, the 1st part is fully kept, 2nd mostly kept, 3rd mostly forgotten.

---

### 2️⃣ Input gate + candidate values:

$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
$ĝ_t = \tanh(W_g \cdot [h_{t-1}, x_t] + b_g)$

- Input gate decides which parts of the candidate info `ĝ_t` to write into `C_t`.
- $ĝ_t$ generates new candidate values to potentially add to the cell state.
- Example:  
    If `i_t = [0.9, 0.1, 0.5]` and `ĝ_t = [0.8, -0.3, 0.5]`,  
    → We mostly add the 1st value, barely add the 2nd, moderately add the 3rd.

---

### 3️⃣ Update the cell state:

$C_t = f_t * C_{t-1} + i_t * ĝ_t$

- Old memory scaled by forget gate ($f_t * C_{t-1}$) + new candidate scaled by input gate ($i_t * ĝ_t$).
- This is how LSTM _remembers and forgets_ at the same time.

---

### 4️⃣ Output gate + hidden state:

$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t * \tanh(C_t)$

- Output gate decides what part of the cell memory to expose as hidden state (what LSTM "says" at this time step).
- The output is a filtered version of the updated memory, passed through `tanh` for bounded values.

---

## **What does each matrix do?**

|Matrix|Shape (if h_dim = hidden units, x_dim = input units)|Purpose|
|---|---|---|
|`W_f`|(h_dim + x_dim, h_dim)|Forget gate weights|
|`W_i`|(h_dim + x_dim, h_dim)|Input gate weights|
|`W_c`|(h_dim + x_dim, h_dim)|Candidate (cell input) weights|
|`W_o`|(h_dim + x_dim, h_dim)|Output gate weights|
|`b_f`, `b_i`, `b_c`, `b_o`|(h_dim,)|Biases for each gate|

- **The same weights are shared across all time steps.**  
- **These weights are learned during training to minimize loss.**

---

## **Intuitive Example**

Imagine you’re reading a movie review:

```
"The movie was not at all boring and quite enjoyable."
```

👉 When LSTM reads "not":

- Forget gate: Let’s reduce previous assumptions.
- Input gate: Let’s consider "not" important to memory.  
    👉 When LSTM reads "boring":
- Input gate: This is crucial, might update memory about sentiment.  
    👉 When LSTM reads "enjoyable":
- Forget gate: Let’s erase negative info.
- Input gate: Strongly add positive info.

---

## **Why this design works**

- The cell state is modified gently at each time step → gradients can flow long distances without vanishing.
- The gates allow dynamic memory management → LSTM _decides_ what matters.
- This makes LSTMs great at learning both **short-term patterns (local word groups)** and **long-term dependencies (sentiment flipping, topic consistency)**.

---

## **Summary**

> **LSTMs enhance RNNs by introducing a memory cell and gates that regulate what to forget, what to add, and what to output — enabling the model to capture both short-term and long-term dependencies without suffering from vanishing/exploding gradients.**

---
---

# DEMONSTRATION :

## **Goal**

Simulate LSTM processing **first word of a sentence**:

- Show how gates and states are calculated
- Use small made-up numbers (easy to follow)
- Keep hidden size = 2 for simplicity

---

## **Sentence**

Let’s use:

```
"This movie is good"
```

We’ll process the word `"This"` at `t = 1`.

---

##  **Setup**

- **hidden size**: 2
- **embedding size**: 2

Assume embedding for `"This"`:

$x_1 = \begin{bmatrix} 0.5 \\ -0.1 \end{bmatrix}$

Initial hidden state:

$h_0 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

Initial cell state:

$C_0 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

---

##  **Weight matrices & biases**

We’ll _fake small values_ for easy math (weights = 0.5, biases = 0).

Since hidden + input = 2 + 2 = 4 (input to gates is 4D vector)

---

### Forget gate

$W_f = \begin{bmatrix} 0.5 & 0.5 & 0.5 & 0.5 \\ 0.5 & 0.5 & 0.5 & 0.5 \end{bmatrix}, \quad$
$b_f = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

### Input gate

$W_i = W_f, \quad b_i = b_f$

### Candidate gate

$W_c = W_f, \quad b_c = b_f$

### Output gate

$W_o = W_f, \quad b_o = b_f$

---

## **Step 1 — Concatenate input**

$z = \begin{bmatrix} h_0 \\ x_1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0.5 \\ -0.1 \end{bmatrix}$

---

## **Step 2 — Forget gate**

$f_1 = \sigma(W_f \cdot z + b_f)$

Let’s compute:

$W_f \cdot z = \begin{bmatrix} 0.5*0 + 0.5*0 + 0.5*0.5 + 0.5*(-0.1) \\ 0.5*0 + 0.5*0 + 0.5*0.5 + 0.5*(-0.1) \end{bmatrix} = \begin{bmatrix} 0.25 - 0.05 \\ 0.25 - 0.05 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.2 \end{bmatrix}$

Apply sigmoid:

$\sigma(0.2) ≈ 0.55$ $
$f_1 = \begin{bmatrix} 0.55 \\ 0.55 \end{bmatrix}$

---

## **Step 3 — Input gate**

$i_1 = f_1 = \begin{bmatrix} 0.55 \\ 0.55 \end{bmatrix}$

(same weights → same result)

---

## **Step 4 — Candidate values**

$\tilde{C}_1 = \tanh(W_c \cdot z + b_c)$

We already computed `W_c · z = [0.2, 0.2]`

Apply tanh:

$\tanh(0.2) ≈ 0.197$ 
$\tilde{C}_1 = \begin{bmatrix} 0.197 \\ 0.197 \end{bmatrix}$

---

## **Step 5 — Update cell state**

$C_1 = f_1 * C_0 + i_1 * \tilde{C}_1$

$\begin{bmatrix} 0.55*0 + 0.55*0.197 \\ 0.55*0 + 0.55*0.197 \end{bmatrix} = \begin{bmatrix} 0.108 \\ 0.108 \end{bmatrix}$

---

## **Step 6 — Output gate**

$o_1 = f_1 = \begin{bmatrix} 0.55 \\ 0.55 \end{bmatrix}$

---

## **Step 7 — Hidden state**

$h_1 = o_1 * \tanh(C_1)$

$\tanh(0.108) ≈ 0.107$

$= \begin{bmatrix} 0.55*0.107 \\ 0.55*0.107 \end{bmatrix} = \begin{bmatrix} 0.059 \\ 0.059 \end{bmatrix}$

---

## **Result after first word**

$\boxed{ C_1 = [0.108, 0.108], \quad h_1 = [0.059, 0.059] }$

---

## **Summary of what happened**

- The gates processed the input `"This"` + previous states.  
- Forget and input gates let some info through (~0.55 weight)  
- Cell state updated slightly (added candidate info scaled by input gate)  
- Hidden state reflected a small positive activation

---

 ## **Processing `"movie"` at t = 2**

---

## **Embedding for `"movie"`**

Let’s assume:

$x_2 = \begin{bmatrix} 0.3 \\ 0.8 \end{bmatrix}$

---

## ⚙ **Concatenate input**

$z = \begin{bmatrix} h_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0.059 \\ 0.059 \\ 0.3 \\ 0.8 \end{bmatrix}$

---

## 1️⃣ **Forget gate**

$f_2 = \sigma(W_f \cdot z + b_f)$

First compute:

$W_f \cdot z = \begin{bmatrix} 0.5*0.059 + 0.5*0.059 + 0.5*0.3 + 0.5*0.8 \\ 0.5*0.059 + 0.5*0.059 + 0.5*0.3 + 0.5*0.8 \end{bmatrix}$

Let’s compute:

$= \begin{bmatrix} 0.0295 + 0.0295 + 0.15 + 0.4 \\ 0.0295 + 0.0295 + 0.15 + 0.4 \end{bmatrix} = \begin{bmatrix} 0.609 \\ 0.609 \end{bmatrix}$

Apply sigmoid:

$\sigma(0.609) ≈ 0.648$ 
$f_2 = \begin{bmatrix} 0.648 \\ 0.648 \end{bmatrix}$

---

## 2️⃣ **Input gate**

Same weights → same W · z

$i_2 = f_2 = \begin{bmatrix} 0.648 \\ 0.648 \end{bmatrix}$

---

## 3️⃣ **Candidate cell values**

$\tilde{C}_2 = \tanh(W_c \cdot z + b_c)$

Since `W_c · z = 0.609`

$\tanh(0.609) ≈ 0.543$ 
$\tilde{C}_2 = \begin{bmatrix} 0.543 \\ 0.543 \end{bmatrix}$

---

## 4️⃣ **Cell state update**

$C_2 = f_2 * C_1 + i_2 * \tilde{C}_2$

Compute:

$= \begin{bmatrix} 0.648*0.108 + 0.648*0.543 \\ 0.648*0.108 + 0.648*0.543 \end{bmatrix}$ 
$=\begin{bmatrix} 0.07 + 0.352 \\ 0.07 + 0.352 \end{bmatrix} = \begin{bmatrix} 0.422 \\ 0.422 \end{bmatrix}$

---

## 5️⃣ **Output gate**

Same weights →

$o_2 = f_2 = \begin{bmatrix} 0.648 \\ 0.648 \end{bmatrix}$

---

## 6️⃣ **Hidden state**

$h_2 = o_2 * \tanh(C_2)$
$\tanh(0.422) ≈ 0.398$
$= \begin{bmatrix} 0.648 * 0.398 \\ 0.648 * 0.398 \end{bmatrix} = \begin{bmatrix} 0.258 \\ 0.258 \end{bmatrix}$

---

## **Result after second word**

$\boxed{ C_2 = [0.422, 0.422], \quad h_2 = [0.258, 0.258] }$

---

## **Summary of what happened**

- `"movie"` contributed more to the cell state (stronger activations due to higher embedding values).  
- The gates let through more info (`~0.65` vs `0.55` last time).  
- The cell state is accumulating context:

```
C_1 ~ 0.1 → C_2 ~ 0.42
```

Hidden state is now stronger:

```
h_1 ~ 0.059 → h_2 ~ 0.258
```

---

## **Final results after whole sentence**

|t|Word|Cell State C_t|Hidden State h_t|
|---|---|---|---|
|1|This|[0.108, 0.108]|[0.059, 0.059]|
|2|movie|[0.422, 0.422]|[0.258, 0.258]|
|3|is|[0.346, 0.346]|[0.184, 0.184]|
|4|good|[0.802, 0.802]|[0.483, 0.483]|

---

## **Summary**

- Each word updated the cell and hidden states, with the cell accumulating information from both current and past context.  
- Gates regulated what info was added or forgotten at each step.  
- Hidden states at each step could be used for classification, tagging, etc.  
- By the end, the model has a rich representation of the full sentence in `C_4` and `h_4`.

---

# **Gradient flow through LSTM**

## Gradients in LSTM flow mainly through:

- The **cell state** pathway (additive updates help preserve gradient)
- The **hidden state** pathway (subject to gates)

Let’s focus on cell state gradient flow:

$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$

Gradient at `C_t`:

$\frac{\partial L}{\partial C_t}$

Gradient at `C_{t-1}`:

$\frac{\partial L}{\partial C_{t-1}} = \frac{\partial L}{\partial C_t} * f_t + \text{other terms}$

---

### **At each time step**

If:

$f_t ≈ 1$

Gradients pass through mostly unchanged

In RNNs, we had:

$h_t = \tanh(W h_{t-1} + U x_t)$

→ gradients multiplied repeatedly by W → can shrink or blow up

---

## **Let’s see how gradient flows**

Let’s pretend:

$\frac{\partial L}{\partial C_4} = \begin{bmatrix} 1.0 \\ 1.0 \end{bmatrix}$

### t=4 → t=3

$\frac{\partial L}{\partial C_3} = 1.0 * f_4 = 1.0 * 0.728 = 0.728$

### t=3 → t=2

$\frac{\partial L}{\partial C_2} = 0.728 * f_3 = 0.728 * 0.552 ≈ 0.402$

### t=2 → t=1

$\frac{\partial L}{\partial C_1} = 0.402 * f_2 = 0.402 * 0.648 ≈ 0.260$

---

## **Key points**

Compare to an RNN with W = 0.5:

$\frac{\partial L}{\partial h_1} = 1.0 * 0.5 * 0.5 * 0.5 = 0.125$

(gradient shrinks exponentially!)

In LSTM:

- Multiplying by $f_t$ (which is **learned to be near 1** when info needs to be preserved!)
- This keeps gradient size reasonable → prevents vanishing
- $f_t$ near 0 when info should be forgotten → gradient gets blocked by design!

---

## **Why exploding gradients are avoided?**

- The forget gate $f_t$, input gate $i_t$, and output gate $o_t$ all have values between 0 and 1 (sigmoid outputs)  
- This limits uncontrolled growth of gradients (since they’re never multiplied by large values repeatedly)
- The cell update is **additive** rather than purely multiplicative:

$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$

so errors can accumulate, not just compound multiplicatively like in RNNs.

---

## **Summary**

In LSTM:

- Gradients through $C_t$ flow like:

$\frac{\partial L}{\partial C_t} = \prod f_t \cdot \frac{\partial L}{\partial C_T}$

- If $f_t$ ≈ 1 → gradient preserved!
- If $f_t$ ≈ 0 → gradient blocked intentionally (forgetting)

Solves:
- **Vanishing gradient:** Gradient doesn’t shrink exponentially if $f_t$ ≈ 1
- **Exploding gradient:** $f_t$, $i_t$, $o_t$ ∈ (0,1) prevents large growth

---

## **Visual interpretation**

```
Loss
 ↓
 ∂L/∂C4 → (×f4)
 ↓
 ∂L/∂C3 → (×f3)
 ↓
 ∂L/∂C2 → (×f2)
 ↓
 ∂L/∂C1
```

Controlled flow thanks to the gates!

---
---


