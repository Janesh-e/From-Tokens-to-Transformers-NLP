# **Long Short-Term Memory (LSTM) Networks: Introduction**

In the world of sequence modeling, **Recurrent Neural Networks (RNNs)** laid the foundation for processing data with temporal dependencies — such as sentences, speech, or time-series signals. RNNs are designed to handle inputs of variable lengths and maintain a form of "memory" through their hidden states, enabling them to connect information across time steps.

However, despite their elegant design, **standard RNNs struggle to capture long-range dependencies in practice**. Let’s explore why, and how **Long Short-Term Memory (LSTM)** networks were introduced as a solution.

---

## **Limitations of Standard RNNs**

1️⃣ **Vanishing Gradient Problem**

- When we train RNNs using backpropagation through time (BPTT), gradients need to flow backward through many time steps.
- If the gradients are repeatedly multiplied by small numbers (due to derivatives of activation functions or weight matrices), they shrink exponentially.
- ➡ Result: Early time steps receive extremely small gradients, preventing the model from learning long-term dependencies.

2️⃣ **Exploding Gradient Problem**

- Conversely, if gradients are repeatedly multiplied by large numbers, they grow exponentially.
- ➡ Result: Training becomes numerically unstable, causing wildly fluctuating loss values and failed convergence.

3️⃣ **Short Memory**

- Even beyond gradient issues, the simple recurrence formula of RNNs struggles to effectively remember information over long sequences.
- The hidden state `h_t` gets overwritten as new inputs arrive, making it hard to preserve important context from the distant past.

---

## **The LSTM Solution**

**Long Short-Term Memory (LSTM)** networks were introduced by _Hochreiter and Schmidhuber (1997)_ to directly address these shortcomings.

**What makes LSTM different?**

- LSTMs introduce a more sophisticated "memory cell" structure with built-in mechanisms to control the flow of information.
- Instead of simply updating the hidden state with each new input, LSTMs _decide_ what to keep, what to forget, and what to output.
- These decisions are made using special structures called **gates**.

---

## **Key Components of LSTM**

|Component|Purpose|
|---|---|
|**Cell state (`C_t`)**|The main "memory" of the network that carries information across time steps with minimal modification. Designed to reduce vanishing gradient risk.|
|**Forget gate**|Decides what part of the cell state to erase (forget irrelevant or outdated info).|
|**Input gate**|Determines which new information should be added to the cell state.|
|**Output gate**|Controls what part of the memory will influence the output at the current time step.|

By combining these gates, the LSTM carefully regulates the flow of information, maintaining long-term memory when necessary and forgetting when appropriate.

---

## **How LSTM solves RNN problems**

|Problem in RNN|How LSTM addresses it|
|---|---|
|Vanishing gradients|Cell state enables gradients to flow unchanged (or minimally changed) across many time steps. Gates help prevent uncontrolled shrinking of gradients.|
|Exploding gradients|Controlled updates via gates prevent gradients from growing exponentially.|
|Poor long-term memory|The cell state can preserve information across long sequences, with gates managing when to update or reset memory.|

---

## **Why LSTMs became the go-to for sequence learning**

✅ They capture **both short-term and long-term dependencies** effectively.  
✅ They enable successful training on long sequences (e.g., long sentences, paragraphs, speech, video).  
✅ They are the foundation for many state-of-the-art models in NLP, speech recognition, and beyond.

---

### **In summary**

> **LSTMs enhance the basic RNN by introducing memory cells and gates that actively control information flow. This architecture overcomes key limitations of standard RNNs, making it possible to model sequences with both short- and long-range dependencies effectively.**

---
---

# **LSTM: Conceptual & Theoretical Understanding**

---

## **The Big Picture**

LSTM (Long Short-Term Memory) networks were created to _solve the memory problem_ in standard RNNs:

- Standard RNNs mix up old and new information at each step → hard to remember distant past.
- LSTMs introduce a **cell state** (the memory) and **gates** (controllers) that regulate information flow:
    -  What to forget
    -  What to add
    -  What to output

Think of LSTM as:
> **A conveyor belt (cell state) running through the network, with gates controlling what is placed on, removed from, or read off the belt at each time step.**

---

## **Core LSTM Components**

Let’s break down what happens at _each time step_:

### 🔹 **Cell State (`C_t`)**

- A highway for memory.
- Runs through all time steps with minor modifications.
- Allows gradients to flow without vanishing or exploding.

---

### 🔹 **Hidden State (`h_t`)**

- The output of the LSTM at the current time step.
- Like RNN’s `h_t`, but computed in a smarter way using the gates.

---

### 🔹 **Gates**

LSTMs have _three_ gates — think of them as "valves" with values between 0 (closed) and 1 (fully open):

|Gate|Purpose|Formula shape|
|---|---|---|
|**Forget gate (`f_t`)**|How much of the previous cell state should we keep?|`f_t = σ(W_f · [h_{t-1}, x_t] + b_f)`|
|**Input gate (`i_t`)**|How much new info should we add to the cell state?|`i_t = σ(W_i · [h_{t-1}, x_t] + b_i)`|
|**Candidate (`ĝ_t`)**|What new candidate values could go into the cell state?|`ĝ_t = tanh(W_g · [h_{t-1}, x_t] + b_g)`|
|**Output gate (`o_t`)**|How much of the cell state should we output?|`o_t = σ(W_o · [h_{t-1}, x_t] + b_o)`|

---

## **Step-by-Step Flow**

Let’s follow a word at position `t`:

At time step `t`, the LSTM cell receives:

- `x_t`: current input (e.g., word embedding at time `t`)
- `h_{t-1}`: hidden state from previous time step
- `C_{t-1}`: cell state from previous time step

We combine `x_t` and `h_{t-1}` (often concatenated or matrix multiplied) for calculations.

---

### 1️⃣ Forget gate:

$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

- `W_f`: weight matrix for forget gate
- `b_f`: bias for forget gate
- `σ`: sigmoid activation → squashes values between 0 and 1

- This produces values (0-1) that decide how much of each piece of `C_{t-1}` to keep.
- `f_t ≈ 1` → keep info
- `f_t ≈ 0` → forget info
- Example: If `f_t = [1, 0.7, 0.2]`, the 1st part is fully kept, 2nd mostly kept, 3rd mostly forgotten.

---

### 2️⃣ Input gate + candidate values:

$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
$ĝ_t = \tanh(W_g \cdot [h_{t-1}, x_t] + b_g)$

- Input gate decides which parts of the candidate info `ĝ_t` to write into `C_t`.
- $ĝ_t$ generates new candidate values to potentially add to the cell state.
- Example:  
    If `i_t = [0.9, 0.1, 0.5]` and `ĝ_t = [0.8, -0.3, 0.5]`,  
    → We mostly add the 1st value, barely add the 2nd, moderately add the 3rd.

---

### 3️⃣ Update the cell state:

$C_t = f_t * C_{t-1} + i_t * ĝ_t$

- Old memory scaled by forget gate ($f_t * C_{t-1}$) + new candidate scaled by input gate ($i_t * ĝ_t$).
- This is how LSTM _remembers and forgets_ at the same time.

---

### 4️⃣ Output gate + hidden state:

$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t * \tanh(C_t)$

- Output gate decides what part of the cell memory to expose as hidden state (what LSTM "says" at this time step).
- The output is a filtered version of the updated memory, passed through `tanh` for bounded values.

---

## **What does each matrix do?**

|Matrix|Shape (if h_dim = hidden units, x_dim = input units)|Purpose|
|---|---|---|
|`W_f`|(h_dim + x_dim, h_dim)|Forget gate weights|
|`W_i`|(h_dim + x_dim, h_dim)|Input gate weights|
|`W_c`|(h_dim + x_dim, h_dim)|Candidate (cell input) weights|
|`W_o`|(h_dim + x_dim, h_dim)|Output gate weights|
|`b_f`, `b_i`, `b_c`, `b_o`|(h_dim,)|Biases for each gate|

- **The same weights are shared across all time steps.**  
- **These weights are learned during training to minimize loss.**

---

## **Intuitive Example**

Imagine you’re reading a movie review:

```
"The movie was not at all boring and quite enjoyable."
```

👉 When LSTM reads "not":

- Forget gate: Let’s reduce previous assumptions.
- Input gate: Let’s consider "not" important to memory.  
    👉 When LSTM reads "boring":
- Input gate: This is crucial, might update memory about sentiment.  
    👉 When LSTM reads "enjoyable":
- Forget gate: Let’s erase negative info.
- Input gate: Strongly add positive info.

---

## **Why this design works**

- The cell state is modified gently at each time step → gradients can flow long distances without vanishing.
- The gates allow dynamic memory management → LSTM _decides_ what matters.
- This makes LSTMs great at learning both **short-term patterns (local word groups)** and **long-term dependencies (sentiment flipping, topic consistency)**.

---

## **Summary**

> **LSTMs enhance RNNs by introducing a memory cell and gates that regulate what to forget, what to add, and what to output — enabling the model to capture both short-term and long-term dependencies without suffering from vanishing/exploding gradients.**

---
---

